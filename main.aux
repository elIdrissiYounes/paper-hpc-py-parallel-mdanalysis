\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Gowers:2016aa,Michaud-Agrawal:2011fu}
\citation{Gowers:2016aa}
\citation{Michaud-Agrawal:2011fu}
\citation{Khoshlessan:2017ab}
\citation{Khoshlessan:2017ab}
\citation{Rocklin:2015aa}
\citation{Rocklin:2015aa}
\citation{Dalcin:2011aa,Dalcin:2005aa}
\citation{Dalcin:2011aa}
\citation{Dalcin:2005aa}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\newlabel{sec:introduction}{{1}{2}{Introduction}{section.1}{}}
\citation{Liu:2010kx,Theobald:2005vn}
\citation{Liu:2010kx}
\citation{Theobald:2005vn}
\citation{Gowers:2016aa}
\citation{Gowers:2016aa}
\@writefile{toc}{\contentsline {section}{\numberline {2}Molecular Dynamics Analysis Applications}{3}{section.2}}
\newlabel{use_cases}{{2}{3}{Molecular Dynamics Analysis Applications}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}MDAnalysis}{3}{subsection.2.1}}
\newlabel{sec:mda}{{2.1}{3}{MDAnalysis}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Root Mean Square Distance (RMSD)}{3}{subsubsection.2.1.1}}
\citation{Sittel:2014aa}
\citation{Sittel:2014aa}
\citation{Dalcin:2011aa,Dalcin:2005aa}
\citation{Dalcin:2011aa}
\citation{Dalcin:2005aa}
\citation{Dalcin:2011aa}
\citation{Dalcin:2011aa}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Dihedral Featurization}{4}{subsubsection.2.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{4}{section.3}}
\newlabel{frameworks}{{3}{4}{Background}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}MPI for Python}{4}{subsection.3.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces RMSD Algorithm\relax }}{5}{algorithm.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:RMSD}{{1}{5}{RMSD Algorithm\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Applications of Global Array}{5}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}MPI and Parallel HDF5}{6}{subsection.3.3}}
\citation{Seyler:2014il}
\citation{Seyler:2014il}
\citation{Khoshlessan:2017ab}
\citation{Khoshlessan:2017ab}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Collective Versus Independent Operations}{7}{subsubsection.3.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{7}{section.4}}
\newlabel{methods}{{4}{7}{Method}{section.4}{}}
\citation{Khoshlessan:2017ab}
\citation{Khoshlessan:2017ab}
\citation{Dalcin:2011aa,Dalcin:2005aa}
\citation{Dalcin:2011aa}
\citation{Dalcin:2005aa}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Notation of our performance modeling}}{8}{table.caption.2}}
\newlabel{tab:notation}{{1}{8}{Notation of our performance modeling}{table.caption.2}{}}
\newlabel{eq:speedup}{{1}{8}{Timing observables}{equation.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Performance Study}{8}{section.5}}
\newlabel{impl_exp}{{5}{8}{Performance Study}{section.5}{}}
\citation{Dalcin:2011aa}
\citation{Dalcin:2011aa}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.0.1}RMSD Benchmarks}{9}{subsubsection.5.0.1}}
\newlabel{sec:RMSD}{{5.0.1}{9}{RMSD Benchmarks}{subsubsection.5.0.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Dihedral Featurization Benchmarks}{9}{subsection.5.1}}
\newlabel{DF}{{5.1}{9}{Dihedral Featurization Benchmarks}{subsection.5.1}{}}
\newlabel{fig:MPIscaling}{{1a}{10}{Scaling total\relax }{figure.caption.3}{}}
\newlabel{sub@fig:MPIscaling}{{a}{10}{Scaling total\relax }{figure.caption.3}{}}
\newlabel{fig:MPIspeedup}{{1b}{10}{Speed-up\relax }{figure.caption.3}{}}
\newlabel{sub@fig:MPIspeedup}{{b}{10}{Speed-up\relax }{figure.caption.3}{}}
\newlabel{fig:MPIranks}{{1c}{10}{Compute \tcomp , IO \tIO , communication \tcomm , ending the for loop $t_{end\_loop}$, opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$, $t_{Overhead2}$ per MPI rank (as described in methods).\relax }{figure.caption.3}{}}
\newlabel{sub@fig:MPIranks}{{c}{10}{Compute \tcomp , IO \tIO , communication \tcomm , ending the for loop $t_{end\_loop}$, opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$, $t_{Overhead2}$ per MPI rank (as described in methods).\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Performance of the RMSD task with MPI which is I/O-bound $\ensuremath  {{t}_{\text  {compute\_final}}}\xspace  /\ensuremath  {{t}_{\text  {IO\_final}}}\xspace  \approx 0.3$. Data are read from the file system (I/O included) and results are communicated back to rank 0 (communications included).\relax }}{10}{figure.caption.3}}
\newlabel{fig:MPIwithIO}{{1}{10}{Performance of the RMSD task with MPI which is I/O-bound $\tcomp /\tIO \approx 0.3$. Data are read from the file system (I/O included) and results are communicated back to rank 0 (communications included).\relax }{figure.caption.3}{}}
\citation{Hintze:1998tw}
\citation{Hintze:1998tw}
\citation{Hintze:1998tw}
\citation{Hintze:1998tw}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Scaling of the \ensuremath  {{t}_{\text  {compute\_final}}}\xspace  and \ensuremath  {{t}_{\text  {IO\_final}}}\xspace  of the RMSD task with MPI.\relax }}{11}{figure.caption.5}}
\newlabel{fig:ScalingComputeIO}{{2}{11}{Scaling of the \tcomp and \tIO of the RMSD task with MPI.\relax }{figure.caption.5}{}}
\newlabel{fig:MPI-dihedral-comm-speedup}{{3c}{12}{Speed-up\relax }{figure.caption.6}{}}
\newlabel{sub@fig:MPI-dihedral-comm-speedup}{{c}{12}{Speed-up\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Performance for the \textbf  {dihedral featurization} workload, which is compute-bound $\ensuremath  {{t}_{\text  {compute\_final}}}\xspace  /\ensuremath  {{t}_{\text  {IO\_final}}}\xspace  \approx 100$, when \emph  {communications are included}.\relax }}{12}{figure.caption.6}}
\newlabel{fig:MPI-dihedral-comm}{{3}{12}{Performance for the \textbf {dihedral featurization} workload, which is compute-bound $\tcomp /\tIO \approx 100$, when \emph {communications are included}.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparison of communication cost for different number of processes over 5 repeats for the \textbf  {dihedral featurization} workload (with \emph  {communications included}).\relax }}{13}{figure.caption.7}}
\newlabel{fig:comparison-t_comm-dihedral}{{4}{13}{Comparison of communication cost for different number of processes over 5 repeats for the \textbf {dihedral featurization} workload (with \emph {communications included}).\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Effect of \ensuremath  {{t}_{\text  {compute\_final}}}\xspace  /\ensuremath  {{t}_{\text  {IO\_final}}}\xspace  on Performance}{13}{subsection.5.2}}
\newlabel{bound}{{5.2}{13}{Effect of \tcomp /\tIO on Performance}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Increased workload (RMSD)}{13}{subsubsection.5.2.1}}
\newlabel{fig:S1_tcomp_tIO_effect}{{5a}{14}{Effect of \tcomp /\tIO on the Speed-Up\relax }{figure.caption.8}{}}
\newlabel{sub@fig:S1_tcomp_tIO_effect}{{a}{14}{Effect of \tcomp /\tIO on the Speed-Up\relax }{figure.caption.8}{}}
\newlabel{fig:S2_tcomp_tIO_effect}{{5b}{14}{Change in the Speed-Up with respect to \tcomp /\tIO for different processor counts\relax }{figure.caption.8}{}}
\newlabel{sub@fig:S2_tcomp_tIO_effect}{{b}{14}{Change in the Speed-Up with respect to \tcomp /\tIO for different processor counts\relax }{figure.caption.8}{}}
\newlabel{fig:E_tcomp_tIO_effect}{{5c}{14}{Change in the efficiency with respect to \tcomp /\tIO for different processor counts\relax }{figure.caption.8}{}}
\newlabel{sub@fig:E_tcomp_tIO_effect}{{c}{14}{Change in the efficiency with respect to \tcomp /\tIO for different processor counts\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Performance change of the RMSD task with MPI with different $\ensuremath  {{t}_{\text  {compute\_final}}}\xspace  /\ensuremath  {{t}_{\text  {IO\_final}}}\xspace  $ ratios. We tested performance for $\ensuremath  {{t}_{\text  {compute\_final}}}\xspace  /\ensuremath  {{t}_{\text  {IO\_final}}}\xspace  $ ratios of 0.3, 12, 21, 30 which correspond to $1\times $ RMSD, $40\times $ RMSD, $70\times $ RMSD, and $100\times $ RMSD respectively (communication is included)\relax }}{14}{figure.caption.8}}
\newlabel{fig:tcomp_tIO_effect}{{5}{14}{Performance change of the RMSD task with MPI with different $\tcomp /\tIO $ ratios. We tested performance for $\tcomp /\tIO $ ratios of 0.3, 12, 21, 30 which correspond to $1\times $ RMSD, $40\times $ RMSD, $70\times $ RMSD, and $100\times $ RMSD respectively (communication is included)\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Communication Cost and Application of Global Array}{14}{subsection.5.3}}
\newlabel{Global-Array}{{5.3}{14}{Communication Cost and Application of Global Array}{subsection.5.3}{}}
\citation{Khoshlessan:2017ab}
\citation{Khoshlessan:2017ab}
\citation{Sehrish:2009aa}
\citation{Sehrish:2009aa}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}I/O Cost}{15}{subsection.5.4}}
\newlabel{I/O}{{5.4}{15}{I/O Cost}{subsection.5.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}Splitting the Trajectories}{15}{subsubsection.5.4.1}}
\newlabel{Splitting}{{5.4.1}{15}{Splitting the Trajectories}{subsubsection.5.4.1}{}}
\newlabel{fig:MPIscaling-ga4py}{{6a}{16}{Scaling total\relax }{figure.caption.9}{}}
\newlabel{sub@fig:MPIscaling-ga4py}{{a}{16}{Scaling total\relax }{figure.caption.9}{}}
\newlabel{fig:MPIspeedup-ga4py}{{6b}{16}{Speed-up\relax }{figure.caption.9}{}}
\newlabel{sub@fig:MPIspeedup-ga4py}{{b}{16}{Speed-up\relax }{figure.caption.9}{}}
\newlabel{fig:MPIranks-ga4py}{{6c}{16}{Compute \tcomp , IO \tIO , communication \tcomm , ending the for loop $t_{end\_loop}$, opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$, $t_{Overhead2}$ per MPI rank (as described in methods).\relax }{figure.caption.9}{}}
\newlabel{sub@fig:MPIranks-ga4py}{{c}{16}{Compute \tcomp , IO \tIO , communication \tcomm , ending the for loop $t_{end\_loop}$, opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$, $t_{Overhead2}$ per MPI rank (as described in methods).\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Performance of the RMSD task with MPI using global array ($\ensuremath  {{t}_{\text  {compute\_final}}}\xspace  /\ensuremath  {{t}_{\text  {IO\_final}}}\xspace  \approx 0.3$). Data are read from the file system (I/O included). All ranks update the global array and rank 0 accesses the whole RMSD array through the global memory address.\relax }}{16}{figure.caption.9}}
\newlabel{fig:MPIwithIO-ga4py}{{6}{16}{Performance of the RMSD task with MPI using global array ($\tcomp /\tIO \approx 0.3$). Data are read from the file system (I/O included). All ranks update the global array and rank 0 accesses the whole RMSD array through the global memory address.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Scaling of the \ensuremath  {{t}_{\text  {compute\_final}}}\xspace  and \ensuremath  {{t}_{\text  {IO\_final}}}\xspace  of the RMSD task with Global Array.\relax }}{17}{figure.caption.10}}
\newlabel{fig:ScalingComputeIO-ga4py}{{7}{17}{Scaling of the \tcomp and \tIO of the RMSD task with Global Array.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}MPI-based Parallel HDF5}{17}{subsubsection.5.4.2}}
\newlabel{HDF5}{{5.4.2}{17}{MPI-based Parallel HDF5}{subsubsection.5.4.2}{}}
\newlabel{fig:MPIscaling-split}{{8a}{18}{Scaling total\relax }{figure.caption.12}{}}
\newlabel{sub@fig:MPIscaling-split}{{a}{18}{Scaling total\relax }{figure.caption.12}{}}
\newlabel{fig:MPIspeedup-split}{{8b}{18}{Speed-up\relax }{figure.caption.12}{}}
\newlabel{sub@fig:MPIspeedup-split}{{b}{18}{Speed-up\relax }{figure.caption.12}{}}
\newlabel{fig:MPIranks-split}{{8c}{18}{Compute \tcomp , IO \tIO , communication \tcomm , ending the for loop $t_{end\_loop}$, opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$, $t_{Overhead2}$ per MPI rank (as described in methods).\relax }{figure.caption.12}{}}
\newlabel{sub@fig:MPIranks-split}{{c}{18}{Compute \tcomp , IO \tIO , communication \tcomm , ending the for loop $t_{end\_loop}$, opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$, $t_{Overhead2}$ per MPI rank (as described in methods).\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Performance of the RMSD task with MPI using global array ($\ensuremath  {{t}_{\text  {compute\_final}}}\xspace  /\ensuremath  {{t}_{\text  {IO\_final}}}\xspace  \approx 0.3$). Data are read from the file system (I/O included). All ranks update the global array and rank 0 accesses the whole RMSD array through the global memory address.\relax }}{18}{figure.caption.12}}
\newlabel{fig:MPIwithIO-split}{{8}{18}{Performance of the RMSD task with MPI using global array ($\tcomp /\tIO \approx 0.3$). Data are read from the file system (I/O included). All ranks update the global array and rank 0 accesses the whole RMSD array through the global memory address.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Scaling of the \ensuremath  {{t}_{\text  {compute\_final}}}\xspace  and \ensuremath  {{t}_{\text  {IO\_final}}}\xspace  of the RMSD task with Global Array.\relax }}{19}{figure.caption.13}}
\newlabel{fig:ScalingComputeIO-split}{{9}{19}{Scaling of the \tcomp and \tIO of the RMSD task with Global Array.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Guidelines on the Parallel Analysis of Three Dimensional Time Series}{19}{section.6}}
\newlabel{guideline}{{6}{19}{Guidelines on the Parallel Analysis of Three Dimensional Time Series}{section.6}{}}
\newlabel{fig:MPIscaling-split-ga}{{10a}{20}{Scaling total\relax }{figure.caption.15}{}}
\newlabel{sub@fig:MPIscaling-split-ga}{{a}{20}{Scaling total\relax }{figure.caption.15}{}}
\newlabel{fig:MPIspeedup-split-ga}{{10b}{20}{Speed-up\relax }{figure.caption.15}{}}
\newlabel{sub@fig:MPIspeedup-split-ga}{{b}{20}{Speed-up\relax }{figure.caption.15}{}}
\newlabel{fig:MPIranks-split-ga}{{10c}{20}{Compute \tcomp , IO \tIO , communication \tcomm , ending the for loop $t_{end\_loop}$, opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$, $t_{Overhead2}$ per MPI rank (as described in methods).\relax }{figure.caption.15}{}}
\newlabel{sub@fig:MPIranks-split-ga}{{c}{20}{Compute \tcomp , IO \tIO , communication \tcomm , ending the for loop $t_{end\_loop}$, opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$, $t_{Overhead2}$ per MPI rank (as described in methods).\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Performance of the RMSD task with MPI using global array ($\ensuremath  {{t}_{\text  {compute\_final}}}\xspace  /\ensuremath  {{t}_{\text  {IO\_final}}}\xspace  \approx 0.3$). Data are read from the file system (I/O included). All ranks update the global array and rank 0 accesses the whole RMSD array through the global memory address.\relax }}{20}{figure.caption.15}}
\newlabel{fig:MPIwithIO-split-ga}{{10}{20}{Performance of the RMSD task with MPI using global array ($\tcomp /\tIO \approx 0.3$). Data are read from the file system (I/O included). All ranks update the global array and rank 0 accesses the whole RMSD array through the global memory address.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Scaling of the \ensuremath  {{t}_{\text  {compute\_final}}}\xspace  and \ensuremath  {{t}_{\text  {IO\_final}}}\xspace  of the RMSD task with Global Array.\relax }}{21}{figure.caption.16}}
\newlabel{fig:ScalingComputeIO-split-ga}{{11}{21}{Scaling of the \tcomp and \tIO of the RMSD task with Global Array.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{21}{section.7}}
\newlabel{concl}{{7}{21}{Conclusion}{section.7}{}}
\newlabel{fig:MPIscaling-hdf5}{{12a}{22}{Scaling total\relax }{figure.caption.17}{}}
\newlabel{sub@fig:MPIscaling-hdf5}{{a}{22}{Scaling total\relax }{figure.caption.17}{}}
\newlabel{fig:MPIspeedup-hdf5}{{12b}{22}{Speed-up\relax }{figure.caption.17}{}}
\newlabel{sub@fig:MPIspeedup-hdf5}{{b}{22}{Speed-up\relax }{figure.caption.17}{}}
\newlabel{fig:MPIranks-hdf5}{{12c}{22}{Compute \tcomp , IO \tIO , communication \tcomm , ending the for loop $t_{end\_loop}$, opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$, $t_{Overhead2}$ per MPI rank (as described in methods).\relax }{figure.caption.17}{}}
\newlabel{sub@fig:MPIranks-hdf5}{{c}{22}{Compute \tcomp , IO \tIO , communication \tcomm , ending the for loop $t_{end\_loop}$, opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$, $t_{Overhead2}$ per MPI rank (as described in methods).\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Performance of the RMSD task with MPI-based parallel HDF5 ($\ensuremath  {{t}_{\text  {compute\_final}}}\xspace  /\ensuremath  {{t}_{\text  {IO\_final}}}\xspace  \approx 0.3$). Data are read from the file system from a shared HDF5 file instead of XTC format (Independent I/O).\relax }}{22}{figure.caption.17}}
\newlabel{fig:MPIwithIO-hdf5}{{12}{22}{Performance of the RMSD task with MPI-based parallel HDF5 ($\tcomp /\tIO \approx 0.3$). Data are read from the file system from a shared HDF5 file instead of XTC format (Independent I/O).\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Scaling of the \ensuremath  {{t}_{\text  {compute\_final}}}\xspace  and \ensuremath  {{t}_{\text  {IO\_final}}}\xspace  of the RMSD task with MPI-based parallel HDF5 (Independent I/O).\relax }}{23}{figure.caption.18}}
\newlabel{fig:ScalingComputeIO-hdf5}{{13}{23}{Scaling of the \tcomp and \tIO of the RMSD task with MPI-based parallel HDF5 (Independent I/O).\relax }{figure.caption.18}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Dihedral Featurization\relax }}{24}{algorithm.2}}
\newlabel{alg:Dihedral}{{2}{24}{Dihedral Featurization\relax }{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Global Array instead of Message-Passing Paradigm\relax }}{25}{algorithm.3}}
\newlabel{alg:GA}{{3}{25}{Global Array instead of Message-Passing Paradigm\relax }{algorithm.3}{}}
\bibstyle{physbiol-natbib}
\bibdata{journals,becksteinlab}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Time necessary for writing the trajectory segments}}{26}{table.caption.20}}
\newlabel{tab:notation}{{2}{26}{Time necessary for writing the trajectory segments}{table.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{26}{appendix.A}}
\newlabel{sec:splitting-timing}{{A}{26}{Appendix}{appendix.A}{}}
\@writefile{toc}{\contentsline {paragraph}{Detailed timing for splitting the trajectories}{26}{table.caption.20}}
\@writefile{toc}{\contentsline {paragraph}{Version of the libraries used for the present study}{26}{section*.21}}
