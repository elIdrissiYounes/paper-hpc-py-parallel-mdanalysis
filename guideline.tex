\label{guideline}
With a sufficient archive of performance statistics presented in previous sections, we can develop heuristics for parallel analysis of MD trajectories in \package{MDAnalysis} on HPC resources.
It should be noted that the proposed guidelines are potentially applicable to data analysis in other Python-based libraries as well.
To achieve near ideal scaling we enforce the following step by step guidelines:

\begin{description}
  \item[\textbf{Heuristic 1}] Calculate compute to I/O (\tcomp/\tIO) ratio and compute to communication (\tcomp/\tcomm) ratio. 
  \begin{description}
  \item[\textbf{Heuristic 1.1}] \tcomp/\tIO ratio determines whether the task is compute bound ($\frac{\tcomp}{\tIO} \gg 1$) or IO bound ($\frac{\tcomp}{\tIO} \ll 1$). 
  As discussed in Section \ref{sec:bound} for I/O bound problems the interference between MPI communication and I/O traffic can be problematic \cite{VMD2013, Kevin2018} and the performance of the task will be affected by the straggler tasks which delay job completion time.  
  \item[\textbf{Heuristic 1.2}] \tcomp/\tcomm ratio determines whether the task is communication bound ($\frac{\tcomp}{\tcomm} \ll 1$) or compute bound ($\frac{\tcomp}{\tIO} \gg 1$).
  \end{description}
  
  \item[\textbf{Heuristic 2}] For $\frac{\tcomp}{\tIO} \geqslant 1$, one needs to consider the following cases: 
  \begin{description}
  \item[\textbf{Heuristic 2.1}] If $\frac{\tcomp}{\tcomm} \gg 1$, the task is compute bound and will scale very well as we showed in Figure \ref{fig:tcomp_tIO_effect}.
  \item[\textbf{Heuristic 2.2}] If $\frac{\tcomp}{\tcomm} \ll 1$, one might consider using Global Arrays to achieve near ideal scaling behavior (Section \ref{Global-Array}). 
  Application of Global Arrays is useful in the sense that it replaces message-passing interface with a distributed shared array where its 
  blocks will be updated by the associated rank in the communication domain (Algorithm \ref{alg:GA}). 
  \end{description}
  
  \item[\textbf{Heuristic 3}] For $\frac{\tcomp}{\tIO} \leqslant 1$ the task is I/O bound and one needs to take the following steps:  
  \begin{description}
    \item[\textbf{Heuristic 3.1}] If there is access to HDF5 format the recommended way will be to use MPI-based Parallel HDF5 (Section \ref{HDF5}). Since converting the XTC file to HDF5 is expensive, if the trajectory file formats are not in HDF5 format then one might prefer to split the trajectories. MD trajectories are often re-analyzed and therefore we suggest to incorporate trajectory conversion into the beginning of standard workflows for MD simulations. 
 Alternatively, it will be a good idea to keep the trajectories in smaller chunks, e.g. when running simulations on HPC resources using Gromacs \cite{Gromacs3, Gromacs1}, users can run their simulations with ``-noappend" option which   will automatically store the output trajectories in small chunks.
    \item[\textbf{Heuristic 3.2}] If there is not access to HDF5, the trajectory file should be split into as many trajectory segments as the number of processes. Splitting the trajectories is cheap (because it can be performed in parallel) (See \ref{sec:splitting-timing}).
    \item[\textbf{Heuristic 3.3}] In case of $\frac{\tcomp}{\tcomm} \ll 1$, appropriate parallel implementation along with \emph{Global Arrays} should be used on the trajectory segments (Section \ref{splitting-traj}) to achieve near ideal scaling.
  \end{description}
\end{description}
