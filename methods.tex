\label{methods}

\subsection{Timing observables}
We evaluate MPI performance based on the RMSD algorithm (\ref{alg:RMSD}). 
The notation for our models is summarized in Table \ref{tab:notation}.
Inside the code, relevant probs were taken and stored. 
We will abbreviate the timings in the following as variables \text{$t_{L_{n}}$} where \text{$L_{n}$} refers to the line number in algorithm \ref{alg:RMSD}.
Similar calculations can be used for all other algorithms.

We directly measured inside our code (in the function \texttt{block\_rmsd()}) the ``I/O'' time for
ingesting the data from the file system into memory, ($t_{I/O}^{frame} = t_{L4}$) and the ``compute'' time per
trajectory frame to perform the computation ($t_{comp}^{frame} = t_{L5}$). 
 \tIO is the summation of ``I/O'' time per frame and \tcomp is the summation of ``compute'' time per frame for all the frames assigned to each rank ($N_{frames}$). 
$t_{end\_loop} = t_{L6}+t_{L7}$ is the time delay between the end of the last iteration and exiting the \texttt{for} loop.
$t_{opening\_trajectory} = t_{L2}+t_{L3}$ is the time which data structures are initialized and topology and trajectory files are opened (problem setup).

$t_{Communication_{MPI}} = t_{L16}$ is the time ``Shuffle'' time to gather (``reduce'') all data from all processor ranks to rank zero.
The total time (for all frames) spent in \texttt{block\_rmsd()} is $t_{\text{RMSD}} = t_{L1} + ...+ t_{L8}$. 
There are parts of the code in \texttt{block\_rmsd()} that are not covered by the detailed timing information of \tcomp and \tIO. 
To measure the un-accounted time we define the ``overheads''.
$t_{Overhead1}$ and $t_{Overhead2}$ are the overhead of the calculations and they should be ideally very small.  
The total time to completion for a single process on $N$ cores is $t_{N}$, which is mathematically equivalent to
$t_{N} \equiv t_{RMSD} + \tcomm$.

\subsection{Performance Measurement}
We also recorded the total time to solution $t_{\text{total}}(N)$ with $N$ MPI processes on $N$ cores (which is effectively
$t_{\text{total}}(N) \approx \max t_{N}$). 
Strong scaling was quantified by calculating the speed-up relative to performance on a single core \text{$S = \frac{t_{\text{total}}(N)}{t_{\text{total}}(1)}$} and efficiency \text{$E = \frac{S}{N}$} (using MPI).

Additionally, we introduce two important performance parameters that determine whether we observe stragglers.
We define the ratio of compute time to I/O time \text{$t_{Compute}$/$t_{IO}$}. 
In the present study, we calculated this ratio using the serial version of our algorithms.
However, one can run the algorithm using several cores and several frames of the trajectory to get an estimation of the compute to I/O ratio. 
It is possible to estimate \text{$t_{Compute}$/$t_{IO}$} serially because, ideally, compute time per frame and I/O time per frame should be the same for different runs using different processes and we have shown this on our previous study \cite{Khoshlessan:2017ab}. Equation \ref{eq:Compute-I/O} shows the definition of compute to I/O ratio and is defined for a single rank.
\obnote{not clear to me what you want to say here ? do you want to say that the average ratio is approximately the ratio of the averages?} \mknote{No. compute time per frame and I/O time per frame should not change in parallel and in serial. Therefore we can say that this ratio can be estimated using serial version of the code}

\begin{gather}
  \label{eq:Compute-I/O}
    \frac{t_{\text{comp}}}{t_{\text{IO}}}=\frac{t_{\text{comp}}^{frame}}{t_{\text{IO}}^{frame}} \approx \frac{\overline{t_{\text{comp}}^{frame}}}{\overline{t_{\text{IO}}^{frame}}} \approx \frac{\frac{\sum_{1}^{N_{\text{frames}}}t_{\text{comp}}^{frame}}{N_{\text{frames}}}}{\frac{\sum_{1}^{N_{\text{frames}}}t_{\text{I/O}}^{frame}}{N_{\text{frames}}}} 
 \end{gather}

and the ratio of compute to communication time is defined by the ratio of total compute time per rank averaged across all ranks to total communication time per rank averaged across all ranks as shown in Table \ref{tab:notation}:

\begin{gather}
  \label{eq:Compute-comm}
       \frac{t_{\text{comp}}}{t_{\text{comm}}}= \frac{\frac{\sum_{1}^{N}\sum_{1}^{N_{\text{frames}}}t_{\text{comp}}^{frame}}{N}}{\overline{t_{\text{comm}}}}  
 \end{gather}
 
All compute codes used in the present study are available from \url{https://github.com/hpcanalytics/supplement-hpc-py-parallel-mdanalysis} under the MIT license. 
 
\begin{table}[ht!]
\centering
\begin{tabular}{c c}
  \toprule
           \bfseries\thead{Item} & \bfseries\thead{Definition}\\
  \midrule
  \midrule
    $N_{\text{frames}}$ & $N_{\text{frames}}^{total}/N$\\  
    $t_{\text{end\_loop}}$ & $t_{L6}+t_{L7}$\\
    $t_{\text{opening\_trajectory}}$ &  $t_{L2}+t_{L3}$ \\
    $t_{\text{comp}}$ & $\sum_{1}^{N_{\text{frames}}}t_{\text{comp}}^{\text{frame}}$\\
    $t_{\text{I/O}}$ & $\sum_{1}^{N_{\text{frames}}}t_{\text{I/O}}^{\text{frame}}$\\
    $t_{\text{all\_frame}}$ & $t_{L4}+t_{L5}+t_{L6}$  \\
    $t_{\text{RMSD}}$ &  $t_{L1} + ...+ t_{L8}$ \\
    $t_{\text{Communication}_{\text{MPI}}}$ &  $t_{L16}$  \\
    $t_{\text{Communication}_{\text{GA}}}$ &  $t_{L5}+t_{L6}+t_{L7}+t_{L8}$  \\
    $t_{\text{Overhead1}}$ & $t_{\text{all\_frame}}-t_{\text{I/O\_final}}-t_{\text{comp\_final}}-t_{\text{end\_loop}}$  \\
    $t_{\text{Overhead2}}$ & $t_{\text{RMSD}}-t_{\text{all\_frame}}-t_{\text{opening\_trajectory}}$  \\
    $t_{N}$ & $t_{\text{RMSD}}+t_{\text{Communication}}$ \\
   \midrule  
    $\overline{t_{\text{comm}}}$ & $\frac{\sum_{1}^{N}t_{\text{communication}}}{N}$ \\
    $t_{\text{total}}$ & $\max t_{N}$ \\
  \bottomrule
\end{tabular}
\caption[Summary of the notation of our performance modeling]
{Summary of the notation of our performance modeling. Relevant probes in the codes are taken and stored,
which we will abbreviate in here as $t_{Ln}$ where {Ln} refers to the line number in the corresponding algorithm. 
$t_{Communication_{MPI}}$ and $t_{Communication_{GA}}$ are both referred to $t_{Communication}$ in the text.
All the timings on the first row are per rank.}
\label{tab:notation}
\end{table}



