\label{methods}

\subsection{Timing observables}
We model MPI performance based on the RMSD algorithm (\ref{alg:RMSD}) and Dihedral Featurization algorithm (\ref{alg:Dihedral}). 
The notation for our models is summarized in Table \ref{tab:notation}.
Inside the code, relevant probs were taken and stored. 
We will abbreviate the timings in the following as variables $t_{Ln}$ where $Ln$ refers to the line number in algorithm \ref{alg:RMSD}.
Similar calculations can be used for all other algorithms.

We directly measured inside our code (in the function \texttt{block\_rmsd()}) the ``I/O'' time for
ingesting the data from the file system into memory, ($t_{I/O}^{frame} = t_{L4}$) and the ``compute'' time per
trajectory frame to perform the computation ($t_{comp}^{frame} = t_{L5}$). 
 \tIO is the summation of ``I/O'' time per frame and \tcomp is the summation of ``compute'' time per frame for all the frames assigned to each rank ($N_{frames}$). 
$t_{end\_loop} = t_{L6}+t_{L7}$ is the time delay between the end of the last iteration and exiting the for loop.
$t_{opening\_trajectory} = t_{L2}+t_{L3}$ is the time which data structures are initialized and topology and trajectory files are opened (problem setup).

$t_{Communication_{MPI}} = t_{L16}$ is the time ``Shuffle'' time to gather (``reduce'') all data from all processor ranks to rank zero.
The total time (for all frames) spent in \texttt{block\_rmsd()} is $t_{\text{RMSD}} = t_{L1} + ...+ t_{L8}$. 
There are parts of the code in \texttt{block\_rmsd()} that are not covered by the detailed timing information of \tcomp and \tIO. 
To measure the un-accounted time we define the ``overheads''.
$t_{Overhead1}$ and $t_{Overhead2}$ are the overhead of the calculations and they should be ideally very small.  
The total time to completion for a single process on $N$ cores is $t_{N}$, which is mathematically equivalent to
$t_{N} \equiv t_{RMSD} + \tcomm$.

\subsection{Performance Measurement}
We also recorded the total time to solution $t_{\text{total}}(N)$ with $N$ MPI processes on $N$ cores (which is effectively
$t_{\text{total}}(N) \approx \max t_{N}$). 
Strong scaling was quantified by calculating the speed-up relative to performance on a single core and efficiency (using MPI).

\begin{gather}
  \label{eq:speedup}
  S = \frac{t_{\text{total}}(N)}{t_{\text{total}}(1)}
\end{gather}

\begin{gather}
  \label{eq:efficiency}
  E = \frac{S}{N}
\end{gather}

Additionally, we introduce two important performance parameters that determines whether we observe stragglers.
We define this parameter as the ratio of compute time to I/O time:

\begin{gather}
  \label{eq:Compute-I/O}
    \frac{\bar{t}_{comp}^{frame}}{\bar{t}_{IO}^{frame}} \approx \frac{\tcomp}{\tIO} 
 \end{gather}

and the ratio of compute to communication time\mknote{This equation should be corrected}:

\begin{gather}
  \label{eq:Compute-comm}
    \frac{\bar{t}_{comp}}{\bar{t}_{communication}} \approx \frac{\tcomp}{\tcomm} 
 \end{gather}
 
\begin{table}[ht!]
\centering
\begin{tabular}{c c}
  \toprule
           \bfseries\thead{Item} & \bfseries\thead{Definition}\\
  \midrule
    $N_{frames}$ & $N_{frames}^{total}/N$\\  
    $t_{end\_loop}$ & $t_{L6}+t_{L7}$\\
    $t_{opening\_trajectory}$ &  $t_{L2}+t_{L3}$ \\
    $t_{comp}$ & $\sum_{1}^{N_{frames}}t_{comp}^{frame}$\\
    $t_{IO}$ & $\sum_{1}^{N_{frames}}t_{I/O}^{frame}$\\
    $t_{all\_frame}$ & $t_{L4}+t_{L5}+t_{L6}$  \\
    $t_{RMSD}$ &  $t_{L1} + ...+ t_{L8}$ \\
    $t_{Communication_{MPI}}$ &  $t_{L16}$  \\
    $t_{Communication_{GA}}$ &  $t_{L5}+t_{L6}+t_{L7}+t_{L8}$  \\
    $t_{Overhead1}$ & $t_{all\_frame}-t_{IO\_final}-t_{comp\_final}-t_{end\_loop}$  \\
    $t_{Overhead2}$ & $t_{RMSD}-t_{all\_frame}-t_{opening\_trajectory}$  \\
    $t_{N}$ & $t_{RMSD}+t_{Communication}$ \\
    $t_{total}$ & $\max t_{N}$ \\
  \bottomrule
\end{tabular}
\caption[Summary of the notation of our performance modeling]
{Summary of the notation of our performance modeling. Relevant probes in the codes are taken and stored,
which we will abbreviate in here as $t_{Ln}$ where {Ln} refers to the line number in the corresponding algorithm. 
$t_{Communication_{MPI}}$ and $t_{Communication_{GA}}$ are both referred to $t_{Communication}$ in the text}
\label{tab:notation}
\end{table}

