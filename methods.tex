\label{methods}

\subsection{Timing observables}
We evaluate MPI performance based on the RMSD algorithm~\ref{alg:RMSD}. 
The notation for our models is summarized in Table~\ref{tab:notation}.
We abbreviate the timings in the following as variables $t_{\text{L}_{\text{n}}}$ where $L_{n}$ refers to the line number in algorithm~\ref{alg:RMSD}.

We directly measured inside our code (in the function \texttt{block\_rmsd()}) the ``I/O'' time for
ingesting the data from the file system into memory, ($t_{\text{I/O}}^{\text{frame}} = t_{\text{L4}}$) and the ``compute'' time per
trajectory frame to perform the computation ($t_{\text{comp}}^{\text{frame}} = t_{\text{L5}}$).
The total I/O time for a rank  $\tIO = \sum_{\text{frame}=1}^{N_{\text{b}}} t_{\text{I/O}}^{\text{frame}}$ is the sum over all I/O times for all the $N_{\text{frames}}$ frames assigned to the rank; similarly, the total compute time for a rank is $\tcomp = \sum_{\text{frame}=1}^{N_{\text{b}}} t_{\text{comp}}^{\text{frame}}$. 
The time delay between the end of the last iteration and exiting the \texttt{for} loop is $t_{\text{end\_loop}} = t_{\text{L6}}+t_{\text{L7}}$.
The time $t_{\text{opening\_trajectory}} = t_{\text{L2}}+t_{\text{L3}}$ measures the problem setup, which includes data structure initialization and opening of topology and trajectory files.
$t_{\text{Communication\_{MPI}}} = t_{\text{L16}}$ is the time to gather (``reduce'') all data from all processor ranks to rank zero.
The total time (for all frames) spent in \texttt{block\_rmsd()} is $t_{\text{RMSD}} = \sum_{i=1}^{8}t_{\text{Li}}$. 
There are parts of the code in \texttt{block\_rmsd()} that are not covered by the detailed timing information of \tcomp and \tIO.

Unaccounted time is considered as ``overhead''. We define
$t_{\text{Overhead1}}$ and $t_{\text{Overhead2}}$ as the overhead of the calculations. 
They should ideally be very small (see Table \ref{tab:notation} for the definition). 
Finally, the total time to completion of a single process, when utilizing $N$ cores for the execution of the overall experiment, is $t_{N}$, as a result $t_{\text{RMSD}} + \tcomm \equiv t_{N}$.

\subsection{Performance Measurement}
We also recorded the total time to solution $t_{\text{total}}(N)$ with $N$ MPI processes on $N$ cores (which is effectively
$t_{\text{total}}(N) \approx \max(t_{N})$). 
Strong scaling was quantified by calculating the speed-up relative to performance on a single core

\begin{equation}
  \label{eq:speedup}
  S = \frac{t_{\text{total}}(N)}{t_{\text{total}}(1)}
\end{equation}
and efficiency
\begin{equation}
  \label{eq:efficiency}
  E = \frac{S}{N}.
\end{equation}
Additionally, we introduced two performance parameters that we will show are indicative of the occurrence of stragglers.
We define the ratio of compute time to I/O time as $t_{\text{Compute}}/t_{\text{IO}}$. 
In the present study, we calculated this ratio using the serial version of our algorithms because compute time per frame and I/O time per frame should ideally be the same for different runs using different processes, as we observed previously~\cite{Khoshlessan:2017ab}.
The ratio of compute to communication time is defined by the ratio of total compute time per rank averaged across all ranks to the total communication time per rank averaged across all ranks 
\begin{gather}
  \label{eq:Compute-comm}
  \frac{\overline{t_{\text{comp}}}}{\overline{t_{\text{comm}}}} = \frac{\frac{1}{N}
    \sum_{\text{rank}=1}^{N} \sum_{\text{frame}=1}^{N_{\text{b}}}t_{\text{comp}}^{\text{frame}}}%
  {\overline{t_{\text{comm}}}},
\end{gather}
as shown in Table~\ref{tab:notation}.
 
All code used in the present study is available in our Github repository. 
 
\begin{table}[ht!]
\centering
\begin{tabular}{c c}
  \toprule
           \bfseries\thead{Item} & \bfseries\thead{Definition}\\
  \midrule
  \midrule
    $N_{\text{b}}$ & $N_{\text{frames}}^{\text{total}}/N$\\  
    $t_{\text{end\_loop}}$ & $t_{\text{L6}}+t_{\text{L7}}$\\
    $t_{\text{opening\_trajectory}}$ &  $t_{\text{L2}}+t_{\text{L3}}$ \\
    $t_{\text{comp}}$ & $\sum_{\text{frame}=1}^{N_{\text{b}}}t_{\text{comp}}^{\text{frame}}$\\
    $t_{\text{I/O}}$ & $\sum_{\text{frame}=1}^{N_{\text{b}}}t_{\text{I/O}}^{\text{frame}}$\\
    $t_{\text{all\_frame}}$ & $t_{\text{L4}}+t_{\text{L5}}+t_{\text{L6}}$  \\
    $t_{\text{RMSD}}$ &  $t_{\text{L1}} + ...+ t_{\text{L8}}$ \\
    $t_{\text{Communication}_{\text{MPI}}}$ &  $t_{\text{L16}}$  \\
    $t_{\text{Communication}_{\text{GA}}}$ &  $t_{\text{L5}}+t_{\text{L6}}+t_{\text{L7}}+t_{\text{L8}}$  \\
    $t_{\text{Overhead1}}$ & $t_{\text{all\_frame}}-t_{\text{I/O\_final}}-t_{\text{comp\_final}}-t_{\text{end\_loop}}$  \\
    $t_{\text{Overhead2}}$ & $t_{\text{RMSD}}-t_{\text{all\_frame}}-t_{\text{opening\_trajectory}}$  \\
    $t_{N}$ & $t_{\text{RMSD}}+t_{\text{Communication}}$ \\
   \midrule  
    $\overline{t_{\text{comp}}}$ & $\frac{1}{N}\sum_{\text{rank}=1}^{N} \sum_{\text{frame}=1}^{N_{\text{b}}}t_{\text{comp}}^{\text{frame}}$ \\
    $\overline{t_{\text{comm}}}$ & $\frac{1}{N}\sum_{\text{rank}=1}^{N}t_{\text{communication}}$ \\
    $t_{\text{total}}$ & $\max t_{N}$ \\
  \bottomrule
\end{tabular}
\caption[Summary of the notation of our performance modeling]
{Summary of the notation of our performance modeling. Relevant probes in the codes are taken and stored,
which we will abbreviate in here as $t_{\text{Ln}}$ where {\text{Ln}} refers to the line number in the corresponding algorithm. 
$t_{\text{Communication\_{\text{MPI}}}}$ and $t_{\text{Communication\_{\text{GA}}}}$ are both referred to $t_{\text{Communication}}$ in the text.
All the timings on the first row are per rank.}
\label{tab:notation}
\end{table}



