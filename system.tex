\label{system}
The test system contained the protein adenylate kinase with 214 amino acid residues and 3341 atoms in total \cite{Seyler:2014il}. 
The trajectory \cite{Seyler:2017aa} was in Gromacs XTC format trajectory (``600x'' in \citet{Khoshlessan:2017ab}) with a size of about 30 GB and 2,512,200
time frames (corresponding to $602.4 \sim \mu s$ simulated time) which represents a typical medium per-frame size but is very long for
current standards.

The experiments were executed on the XSEDE Supercomputers: \emph{SDSC Comet}, 
\emph{PSC Bridges} and \emph{SuperMIC}. SDSC Comet is a 2.7 PFlop/s cluster with 
6,400 compute nodes in total. The standard compute nodes consist of Intel Xeon 
E5-2680v3 processors, 128 GB DDR4 DRAM (64 GB per socket). The network topology 
is 56 Gbps FDR Infini-Band (IB).

PSC Bridges is a 1.35 PFlop/s cluster with four types of computational nodes. Bridges 
computational nodes supply 1.3018 PFlop/s and 274 TiB RAM. The Regular Shared 
Memory (RSM) nodes consist of Intel Haswell (E5-2695 v3) processors, 128 GB DDR4 
DRAM (64 GB per socket). The network topology is 12.37 Gbps Omni-Path Architecture 
(OPA).

LSU SuperMIC offers 360 compute nodes with Ivy Bridge Intel processors (E5-2680).
Each node has 64 GBs DDR3 RAM. SUperMIC's nodes are connected 56 Gbps Infiniband 
Network. In addition, it offers 20 hybrid nodes which provide an NVIDIA GPU. SuperMIC's
peak performance is measured at 557 TFlop/s.
All the experiments are performed using standard compute nodes on Comet, PSC Bridges and SuperMIC respectively in the present study.

\begin{table}[ht!]
	\centering
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{c c c c c c c c}
			\toprule
			\bfseries\thead{Cluster} & \bfseries\thead{Nodes} & \makecell{\bfseries\thead{Number \\of Nodes}} & \bfseries\thead{CPUs} &  \bfseries\thead{RAM} & \bfseries\thead{Network Topology} & \makecell{\bfseries\thead{Scheduler and  \\ Resource Manager}}\\
			\midrule
			\bfseries SDSC Comet & Compute & 6400 & \makecell{2 Intel Xeon (E5-2680v3) CPUs \\ 12 cores/CPU, 2.5 GHz} &128 GB DDR4 DRAM & 56 Gbps IB & SLURM\\
			\bfseries PSC Bridges & RSM & 752 & \makecell{2 Intel Haswell (E5-2695 v3) CPUs \\14 cores/CPU, 2.3 GHz} & 128 GB, DDR4-2133Mhz & 12.37 Gbps OPA & SLURM\\
			\bfseries SuperMIC & Standard & 360 & \makecell{2 Intel Ivy Bridges (E5-2680) CPUs \\10 cores/CPU, 2.8GB GHz} & 64 GB, DDR3-1866Mhz  & 56 Gbps IB & PBS\\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption[System configuration used in the present study]
	{List of benchmarked clusters and their system configuration}
	\label{tab:sys-config}
\end{table}

\subsection{Installing libraries on multi-user HPC systems}
\obnote{Include table with software and versions; add a paragraph in which you mention software, summarize how it was compiled, and reference the table.}
\mknote{Are the following enough?}
Different packages and libraries are used in the present study in order to achieve the desired performance. 
However, getting scientific libraries installed can be a very challenging task.
\gpnote{As far as, I know compiling is the standard procedure for highly optimized libraries. Dependencies change since systems change. }\mknote{Yes, dependency can change by system change. But when a package has a lot of dependencies to other packages and libraries that can make managing all those dependices and compilation more challenging and difficult}\gpnote{Are you implying that it shouldn't be challenging? If yes in what way? When a system is engineered and documented no-one can anticipate a specific use, unless that is a use case from the beginning. All installation procedures use are the standard ones, regardless if we like them or not. There are always a lot of dependencies for highly optimized libraries. Do you have a suggestion? Do you have at least one reference to support this argument?}
Lack of documentation and good software engineering practices, non-standard installation procedures, and lots of dependencies can contribute to the challenge of getting the libraries to work \gpnote{Citation Missing}[???].
Scientists mostly care about the science and they are often not software engineers or system administrators. 
\gpnote{I do not think this is a case. I remember at some point I shared a google doc to SPIDAL with all the packages that XSEDE resources offered. There is a good number of frameworks offered and preinstalled.}\mknote{Yes, but this is not necessarily the case for all libraries. For example for parallel hdf5, GA, MDAnalysis, Dask, etc there was nothing in the package manager and both of us had to build them from source. Therefore I do not think this is always the case.}\gpnote{So are you asking for a system administrator to anticipate what a user might need and have it there already? There is no clear message here.}
Therefore, the libraries and tools they need should be easily accessible. 
This is not always the case, though.

In fact, many domain specific packages are not available through package manager in supercomputers and as a result,
we spent considerable amount of time getting packages dependencies to work in the process of our performance study.
As a result, we provide detail information on how we managed to build these libraries.
This will let future works spend least amount of time for this purpose. 
Detailed information regarding the version of each library, its dependencies, the quality of its documentation, the time necessary for building and installing the packages are given in Table \ref{tab:version}. 

\begin{table}[ht!]
\centering
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{c c c c c c c}
  \toprule
            \bfseries\thead{Package} & \bfseries\thead{Version} & \bfseries\thead{Description} & \bfseries\thead{Time to Result} & \bfseries\thead{Documentation} & \bfseries\thead{Installation} & \bfseries\thead{Dependencies}\\
  \midrule
   \bfseries GCC & 4.9.4 & GNU Compiler Collection & Fast & Excellent & \makecell{via configuration \\files, environment \\or command line options, \\ minimal configuration \\ is required} &--\\
   \bfseries OpenMPI & 1.10.7 & MPI Implementation & Fast & Excellent & \makecell{via configuration \\ files, environment \\or command line options, \\ minimal configuration \\ is required} &--\\
   \bfseries Global Array & 5.6.1 & Global Array & Slow & Good & \makecell{via configuration files, environment \\or command line options, \\ several optional configuration\\ settings available} &\makecell{MPI 1.x/2.x/3.x \\ implementation like \\ MPICH or Open MPI \\ built with shared/dynamic\\ libraries, GCC}\\
   \bfseries MPI4py & 3.0.0 & MPI for Python & Very Fast & Excellent & Conda Installation &\makecell{Python 2.7, \\or above, \\ MPI 1.x/2.x/3.x  \\ implementation like \\ MPICH or Open-MPI \\built with shared/dynamic \\libraries, Cython}\\
   \bfseries GA4py & 1.0 & Global Array for Python & Fast & Average & Python Setuptools &\makecell{Python 2.7, \\or above, \\ MPI 1.x/2.x/3.x  \\implementation like \\ MPICH or Open-MPI \\ built with shared/dynamic \\libraries, Cython,  \\MPI4py, Numpy} \\
   \bfseries PHDF5 & 1.10.1 & Parallel HDF5 & Slow & Excellent & \makecell{via configuration files, environment \\or command line options, \\ several optional configuration\\ settings available} &\makecell{MPI 1.x/2.x/3.x  \\ implementation like \\ MPICH or Open-MPI,  \\GNU, MPIF90,  \\MPICC, MPICXX}\\
   \bfseries H5py &  2.7.1 & Pythonic wrapper around the HDF5 & Very Fast & Excellent & Conda Installation & \makecell{Python 2.7, or above,\\ PHDF5, Cython}\\    
   \bfseries MDAnalysis & 0.17.0 & \makecell{Python library  \\ to analyze  \\trajectories from  \\MD simulations} & Very Fast & Excellent & Conda Installation & \makecell{Python $>=$2.7, or $<$3,\\ Cython, GNU, \\Numpy}\\
  \bottomrule
\end{tabular}
\end{adjustbox}
\caption[Version of the packages used in the present study]
{Detailed comparison on the dependency and installation of different packages used in the present study on multi-user HPC systems}
\label{tab:version}
\end{table}

From the libraries given in Table \ref{tab:version}, \emph{MPI4py}, \emph{H5py}, \emph{MDAnalysis} were the easiest to build. 
Users are able to get these packages installed through the \emph{Conda} package support.
\emph{OpenMPI}, \emph{GCC}, can be easily configured and installed.
The configure script supports a lot of different command line options, but the support for these libraries is very strong, they are widely used and the excellent documentation and discussion mailing lists provide users with great resources for consult, troubleshooting and tracking issues.

\emph{Global Array}, and \emph{PHDF5} have much slower installation especially because the libraries are built from source and variable configure options 
are required to support specific network interconnects and back-end run-time environments.
\emph{GA4py} has only one release, and does not provide users with strong documentation and there are still room for improvement for this package. 

We performed our benchmark on several HPC resources and therefore, we had to install all the related packages and tools on all resources.
However, there are always differences in the resources because their set up and architectures differ from each other. 
For example, on SuperMIC although tool installation was done in the same way as Comet and also passed initial testing, the execution did not distribute the processes to all nodes. 
This was due to the fact that our custom OpenMPI installation did not correctly parse the node list offered by SuperMIC to our job. 
Thus, we had to manually pass the node lists to MPIRUN. 
In addition, we found that the loaded modules, along with library path changes, did not propagate to all nodes from our OpenMPI installation. 
OpenMPI's execution engine could not \mknote{Giannis, could or could not?}\gpnote{fixed} access the correct libraries and was not able to launch the processes correctly. 
Reinstalling OpenMPI with enabling the flag to use the Open Run-Time Environment (ORTE) by default and including the OpenMPI installation to BASHRC allowed for correct execution.
 
Overall, the installation was successful on all clusters and we were able to observe similar
performances (will be discussed in the result section) which shows the applicability of the 
libraries for achieving near ideal scaling. 

