\label{use_cases}

\subsection{MDAnalysis}
Simulation data exist in trajectories in the form of three dimensional time series (atoms positions and velocities), and these come in a plethora of different and idiosyncratic file formats. 
\package{MDAnalysis} is a widely used open source Python library to analyze these trajectory files with an object oriented interface. 
The package is written in Python (compatible with version 2.7 and 3.4+), with time critical code in C/Cython. 
\label{sec:mda}

\subsubsection{Root Mean Square Distance (RMSD)}
The calculation of the root mean square distance (\textbf{RMSD}) for C$_{\alpha}$ atoms after
optimal superposition with the QCPROT algorithm \cite{Liu:2010kx,Theobald:2005vn} is commonly required in the analysis of molecular dynamics simulations (Algorithm \ref{alg:RMSD}). 
The task used for the purpose of our benchmark is the $RMSD=\sqrt{\frac{1}{N}\sum_{i=1}^{N}\delta_{i}^{2}}$ implemented in MDAnalysis.analysis.rms module where $\delta_{i}$ is the distance between atom i and a reference structure (implemented in Cython \cite{Gowers:2016aa}).  
This function computes the RMSD between two sets of coordinates using the fast QCPROT algorithm to optimally superimpose two structures and then calculates the RMSD. 

To this, the protein structure (selected C$_{\alpha}$ atoms) in the initial frame will be considered as the reference and as the mobile group at other time steps. 
The superposition is done in the following way:
First, the mobile group is translated so that its center of mass coincides with the one of reference. 
Second, a rotation matrix is computed that spatially aligns the mobile group to reference which minimizes the RMSD between the coordinates of the mobile group and reference structure. 
Finally, all atoms in mobile group are shifted and rotated.
For each frame, a non-negative floating point number is calculated and the final result is a time-series of the RMSD. 
RMSD values show how rigid the domains in a protein structure are, during the transition. 
The order of complexity for RMSD algorithm \ref{alg:RMSD} is $\mathcal{O}(T \times N^{2})$ \cite{rmsd} where T is the number of frames in the trajectory and N the number of particles in a 
frame.

\begin{algorithm}[ht!]
	\scriptsize
	\caption{MPI-parallel Multi-frame RMSD Algorithm}
	\label{alg:RMSD}
	\hspace*{\algorithmicindent} \textbf{Input:} \emph{size}: Total number of frames \\
	\hspace*{\algorithmicindent} \emph{ref}: mobile group in the initial frame which will be considered as reference \\
	\hspace*{\algorithmicindent} \emph{start \& stop}: Starting and stopping frame index\\
	\hspace*{\algorithmicindent} \emph{topology \& trajectory}: files to read the data structure from \\
	\hspace*{\algorithmicindent} \textbf{Output:} Calculated RMSD arrays
	\begin{algorithmic}[1]
		\Procedure{$Block\_RMSD$}{topology, trajectory, $ref$, start, stop}                       
		\State u $\leftarrow$ Universe(topology, trajectory)\Comment{u hold all the information of the physical system}
		\State $g$ $\leftarrow$ u.frames[start:stop]
		\For{$\forall iframe$ in $g$}
		\State $results[iframe] \leftarrow RMSD(g, ref)$
		\EndFor
		\State \Return results
		\EndProcedure
		\\        
		\State MPI Init
		\State rank $\leftarrow$ rank ID
		\State index $\leftarrow$ indices of mobile atom group
		\State xref0 $\leftarrow$ Reference atom group\textsc{\char13}s position
		\State out $\leftarrow$ Block\_RMSD(topology, trajectory, xref0, start=start, stop=stop)
		\\
		\State Gather(out, RMSD\_data, rank\_ID=0)
		\State MPI Finalize
	\end{algorithmic}
\end{algorithm}

\subsubsection{Dihedral Featurization}
As a real-world compute-bound task we investigated \textbf{Dihedral featurization} \cite{Sittel:2014aa} (Algorithm \ref{alg:Dihedral}) whereby a time series of
feature vectors consisting of the two backbone dihedral angles per residue ($\phi_{i}$ and $\psi_{i}$) is calculated for all 212
non-terminal residues. For each frame, an array of dihedral angles is calculated where for later convenience, an angle $\theta_{i}$ is
actually represented as $(\cos\theta_{i}, \sin\theta_{i})$. 
The order of complexity for Dihedral featurization algorithm (Algorithm \ref{alg:Dihedral}) is $\mathcal{O}(T \times N)$. 
\obnote{Write out the calculation, based on the 4 atoms}
\mknote{What calculations? how residues are converted to dihedrals? or the procedure we perform in the algorithm? that one is explained in algorithm 2}

\begin{algorithm}[ht!]
	\scriptsize
	\caption{MPI-parallel Multi-frame Dihedral Featurization Algorithm}
	\label{alg:Dihedral}
	\hspace*{\algorithmicindent} \textbf{Input:} \emph{mobile}: the desired atom groups to perform RMSD on them \\ 
	\hspace*{\algorithmicindent} \emph{start \& stop}: that tell which block of trajectory (frames) is assigned to each rank \\
	\hspace*{\algorithmicindent} \emph{topology \& trajectory}: files to read the data structure from \\
	\hspace*{\algorithmicindent} \textbf{Output:} Calculated Dihedral Angles
	\begin{algorithmic}[1]
		\Procedure{$angle2sincos$}{x}     
		\State \Return $\cos{x},\: \sin{x}$
		\EndProcedure
		\\
		\Procedure{$residues\_to\_dihedrals$}{residues}
		\State List angles
		\For{$\forall res$ in $residues$}
		\State Append $(\phi (res),\: \psi (res))$ in angles
		\EndFor
		\State \Return angles
		\EndProcedure
		\\
		\Procedure{$featurize\_dihedrals$}{dihedrals}
		\State List angles
		\For{$\forall dihedral$ in $dihedrals$}
		\State Append value of $dihedral$ in angles
		\EndFor
		\State \Return \texttt{angle2sincos(angles)}
		\EndProcedure
		\\
		\Procedure{$Block\_Dihedral$}{topology, trajectory, $ref$, start, stop}                       
		\State u $\leftarrow$ Universe(topology, trajectory)
		\State $g$ $\leftarrow$ u.frames[start:stop]
		\State List $results$
		\For{$\forall frame$ in $g$}
		\State $D_{angles} \leftarrow featurize\_dihedrals(dihedrals)$
		\State Append $D_{angles}$ in $results$ 
		\EndFor
		\EndProcedure
		\\      
		\State MPI Init 
		\State residues $\leftarrow$ residues of mobile atom group
		\State dihedrals $\leftarrow$ residues\_to\_dihedrals(residues)
		\State rank $\leftarrow$ rank ID
		\State index $\leftarrow$ indices of mobile atom group
		\State xref0 $\leftarrow$ Reference atom group\textsc{\char13}s position
		\State out $\leftarrow$ Block\_Dihedral(topology, trajectory, xref0, start=start, stop=stop)
		\\
		\State Gather(out, RMSD\_data, rank\_ID=0)
		\State MPI Finalize
	\end{algorithmic}
\end{algorithm}

\subsection{MPI for Python \package{mpi4py}}
MPI for Python (\package{mpi4py}) is a Python wrapper written over Message Passing Interface (MPI) standard and allows any Python program to employ multiple processors \cite{Dalcin:2011aa, Dalcin:2005aa}.
Python has several advantages that makes it a very attractive language including rapid development of small scripts and code prototypes as well as large applications and highly portable and reusable modules and libraries.
\gpnote{I do not understand this sentence. What do you mean by other factors?}\mknote{I say other factors like LOC, number of function invocation, ..}
In addition, Python\textsc{\char13}s interactive nature, and other factors like lines of codes (LOC), number of function invocation, and development time adds to its attractiveness and clarifies why it is a good investment to extend Python use to message-passing parallel programming applications.
Based on the efficiency tests \cite{Dalcin:2011aa, Dalcin:2005aa}, the performance degradation due to using \package{mpi4py} is not prohibitive and the overhead introduced by \package{mpi4py} is far smaller than the overhead associated to the use of interpreted versus compiled languages \cite{GAiN}.
In addition, there are works on improving the communication performance in \package{mpi4py} and it shows minimal overheads compared to C code if efficient raw memory buffers are used for communication \cite{Dalcin:2011aa}.

\subsection{Applications of Global Array}
Global array toolkit provides users with a language interface that allows them to distribute data while maintaining the type of global index space and programming syntax similar to what is available when programming on a single processor \cite{GA}.

Global array (GA) toolkit allows manipulating physically distributed dense multi-dimensional arrays without explicitly defining communication and synchronization between processes.
Global Arrays in NumPy (GAiN) extends GA to python through Numpy \cite{GAiN}. 
The basic components of the Global Arrays toolkit are function calls to create global arrays ($ga\_create$), copy data to ($ga\_put$), from ($ga\_get$), and between global arrays ($ga\_distribution$, $ga\_scatter$), and identify and access the portions of the global array data that are held locally ($ga\_access$). 
In addition, there are also functions to destroy arrays ($ga\_destroy$) and free up the memory originally allocated to them \cite{GAiN}.

The global array itself is physically located in the local memory space of each process \cite{GA}. 
User can get a pointer to this memory by using ga\_access function or one of its variants.
Using this pointer it is possible to directly modify the data that is local to each process.
The GA library keeps track of all these memory locations by recording a list\gpnote{Is this list distributed of centralized? Which process is the owner?}\mknote{All the processes have access to it and in that way will get aware of any change in the data of other processes} of them when a global array is created. 
When a process tries to access a block of data, it first does a decomposition of the request into individual blocks representing the contribution to the total request from the data held locally on each process \cite{PNNL:2018}. 
The requesting process then makes individual requests to each of the remote processes. 
The requests are implemented using the native one-sided semantics inside the the infini-band Verbs library. 
OpenIB/infini-band uses SHMEM (Symmetric Hierarchical MEMory) parallel computing library \gpnote{You want to avoid language that is OS specific. You say in the introduction that scientist are not system administrators. sys5 shmem is very specific even for me.}\mknote{tried to make it less specific} within the node and will use the infini-band network card to communicate between nodes.
Algorithm \ref{alg:GA} describes RMSD algorithm in combination with the global array.
In this algorithm, we use global array instead of message passage paradigm to see if we can reduce communication cost. 

\begin{algorithm}[ht!]
	\scriptsize
	\caption{MPI-parallel Multi-frame RMSD using Global Arrays}
	\label{alg:GA}
	\hspace*{\algorithmicindent} \textbf{Input:}\emph{size}: Total number of frames assigned to each rank $N_{b}$\\
	\hspace*{\algorithmicindent} \emph{g\_a}: Initialized global array \\
	\hspace*{\algorithmicindent} \emph{xref0}: mobile group in the initial frame which will be considered as rerference \\
	\hspace*{\algorithmicindent} \emph{start \& stop}: that tell which block of trajectory (frames) is assigned to each rank \\
	\hspace*{\algorithmicindent} \emph{topology \& trajectory}: files to read the data structure from \\
	\hspace*{\algorithmicindent}\textbf{Include:} $Block\_RMSD$ from Algorithm \ref{alg:RMSD}
	\begin{algorithmic}[1]
		
		\State bsize $\leftarrow$ ceil(trajectory.number\_frames / size)
		\State g\_a $\leftarrow$ ga.create(ga.C\_DBL, [bsize*size,2], "RMSD")
		\State buf $\leftarrow$ np.zeros([bsize*size,2], dtype=float)
		\State out $\leftarrow$ Block\_RMSD(topology, trajectory, xref0, start=start, stop=stop)
		\State ga.put(g\_a, out, (start,0), (stop,2))
		\If{rank == 0}
		\State buf $\leftarrow$ ga.get(g\_a, lo=None, hi=None)
		\EndIf
	\end{algorithmic}
\end{algorithm}

\subsection{MPI and Parallel HDF5}
MPI-based applications work by launching multiple parallel instances of the Python interpreter which communicate with each other via the MPI library. 
HDF5\gpnote{This is a very nice oportunity to introduce HDF5. More details about HDF5 would be nice. If you assume that the reader does not know MPI4py, most probably will not know HDF5} \mknote{Previously I had more detail in this section and Oliver suggested to summarize and avoid too much detail. What type of info do you think will be good to be added here?}
itself handles nearly all the details involved with coordinating file access through MPI library.
This is advantageous to avoid multiple processes to compete over accessing the same file on disk\gpnote{I am not sure what you want to say here. Are the processes accessing the file atomically, i.e. when one process access it all others are blocked? Doesn't that defeat the purpose of parallel I/O}.\mknote{I want to say when we use parallel IO there is no simultaneous access to the same file anymore. When one process access it the others are not blocked otherwise why parallel IO should give us performance?}
In fact in python, MPI-IO opens shared files with a mpio driver. 
In addition,  MPI communicator should be supplied as well and the users also need to follow some constraints for data consistency \cite{pythonhdf5}.

\subsubsection{Collective Versus Independent Operations} 
MPI has two flavors of operation: collective, which means that all processes have to participate in the same order, and independent, which means each process can perform the operation or not and the order also does not matter  \cite{pythonhdf5}.
With HDF5, modifications to file metadata must be done collectively and although all processes perform the same task, they do not wait until the others catch up \cite{pythonhdf5}. 
Other tasks and any type of data operations can be performed independently by processes.
In the present study, we use independent operations.
