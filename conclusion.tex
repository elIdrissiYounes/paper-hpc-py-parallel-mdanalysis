\label{concl} 
There are currently many freely available libraries for the analysis and processing of molecular dynamics trajectories.
However, dramatic increases in the size of trajectories combined with the serial nature of these libraries necessitates 
use of state of the art high performance computing tools for rapid analysis of these trajectory files. 

\package{MDAnalysis} is a package for post-processing of MD trajectories; however, it does not currently provide parallel analysis of these trajectories.
In this study, we looked at the \emph{straggler} problem that we previously faced in an attempt for parallel analysis of these trajectories in \package{MDAnalysis} \cite{Khoshlessan:2017ab}.
To this aim, we tested our benchmark on RMSD (I/O bound) algorithm in \package{MDAnalysis}.

Our initial analysis showed that \tcomp/\tIO and \tcomp/\tcomm ratios have a crucial effect on performance. 
An I/O bound workload (e.g. RMSD task, $\tcomp/\tIO \approx 0.3$) does not scale due to the appearance of \emph{stragglers}. 

Different factors like opening the trajectory file, or other sources of overheads can be responsible for observing \emph{stragglers} for I/O bound workload.
But, for the I/O bound workload, both communication and I/O appeared to be the main scalability bottlenecks when using a shared file.
Besides, our data shows that when we remove I/O, communication effect is less pronounced and scaling is much better as compared to the case that we have I/O (Figure \ref{fig:MPIwithoutIO}).
These results also confirm the fact that \emph{stragglers might be due to the competition between MPI and the Lustre file system on the shared Infini-band interconnect}.  
Since I/O traffic must compete with MPI messages and other traffics, it can be difficult to achieve peak single-node I/O rates for an arbitrary file when the system is fully loaded with other jobs \cite{VMD2013, Kevin2018}. 

%% OB: maybe add here somewhere; moved from experiments to here
%%
%%
% Given the results for $X\times$ RMSD (Figure \ref{fig:tcomp_tIO_effect}) and without I/O (Figure \ref{fig:MPIwithoutIO}) it seems possible that MPI competes with Lustre on the same network interface, which would explain why communication appears to be primarily a problem in the presence of I/O when \tcomp/\tIO is small.
% In fact, these results suggest that decreasing the I/O load relative to the compute load should open up the network for communication.
% This can be confirmed from other previous studies as well \cite{VMD2013, Kevin2018}. % TODO: cite 2018 CPPC paper 


Communication time, \tcomm, could also take much longer for \emph{stragglers} than for ``normal'' MPI ranks when I/O has to be performed through a shared trajectory file (Figure \ref{fig:MPIranks}). 
Additionally, the number of I/O requests is a function of number of frames in the trajectory. 
Therefore, the frequency of sending the I/O requests makes a difference for I/O bound task and compute bound task with the same number of frames per trajectory.
For sufficiently large per-frame compute workload, I/O interferes much less often with communication than an I/O bound task.
\obnote{citations? How can you back up this statement?}\mknote{I believe the results suggest this. Do you think this needs to be validated or confirmed somehow?}
This is why both communication and I/O appeared to prevent us from achieving the near ideal scaling for an I/O bound task.

It should be also noted that, for $\frac{\tcomp}{\tcomm} \gg 1$ effect of communication was less pronounced, and for $\frac{\tcomp}{\tIO} \gg 1$ there is less competition over accessing the shared trajectory file.
We showed this effect by changing \tcomp/\tIO and \tcomp/\tcomm ratios.

We were able to achieve much better performance in our RMSD benchmark when we used Global Arrays toolkit instead of message-passing interface for communication. 
Using Global Arrays, we did not observe any delayed task due to communication (Figure \ref{fig:MPIwithIO-ga4py}) and it significantly reduced the communication cost. 
However, reducing communication cost was not enough for achieving near ideal scaling because I/O remains a bottleneck.
Therefore, for I/O bound tasks we employed mechanisms to overcome \emph{stragglers}. 
We showed several approaches to improve I/O scaling.
We were able to improve I/O through splitting the shared trajectory file (Figure \ref{fig:MPIwithIO-split}) and MPI-based parallel I/O through HDF5 file (Figure \ref{fig:MPIwithIO-hdf5}). 
% OB: add H5MD trajectory format to MDAnalysis
%     https://github.com/MDAnalysis/mdanalysis/issues/762
%     https://github.com/pdebuyl/pyh5md
In both cases, we were able to achieve near ideal scaling.
With splitting the trajectories, effect of communication was still apparent on the performance which was because $\frac{\tcomp}{\tcomm} \ll 1$; however together with 
Global Arrays toolkit we could achieve near ideal scaling (Figure \ref{fig:MPIwithIO-split}).
\obnote{I actually do not understand why we need GA for splitting but not for parallel MPI.}
\mknote{I believe this is because in splitting we still have some sort of IO requests contention. The main reason might be because with parallel IO type of IO requests is totally different as opposed to what we have for normal IO.
ofcourse this is only a hypothesize and need to be validated. One other point is that in phdf5 both IO and communication are using MPI, but in splitting the trajectory IO and MPI communication are separate and they interfere with each other.}

All the above strategies, provide guidelines for parallel analyses on data generated from MD simulations in \package{MDAnalysis} library.
The analysis indicates that it is feasible to run an I/O bound task on HPC resources and achieve near ideal scaling behavior up to 8 nodes.
In addition, we have examined all these benchmarks on several HPC resources in order to make sure that they are independent of the resources.

