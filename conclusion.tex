\label{concl} 
There are currently many freely available libraries for the analysis and processing of three-dimensional time series.
However, dramatic increases in the size of trajectories combined with the serial nature of these libraries necessitates 
use of state of the art high performance computing tools for rapid analysis of these time series. 

\package{MDAnalysis} is a package for post-processing of MD trajectories; however, it does not provide parallel analysis of these trajectories.
In this study, we looked at the \emph{straggler} problem that we previously faced in an attempt for parallel analysis of these trajectories in \package{MDAnalysis} \cite{Khoshlessan:2017ab}.
To this aim, we tested our benchmark on RMSD (I/O bound) and Dihedral featurization (compute bound) algorithms in \package{MDAnalysis}.

Our initial analysis showed that the ratio between compute load to I/O load and compute load to communication load have a crucial effect on the performance. 
For sufficiently large per-frame workloads ($\tcomp/\tIO \approx 100$), close to ideal scaling was achievable (Figure \ref{fig:comparison-t_comm-dihedral}).
However, the I/O bound workload ($\tcomp/\tIO \approx 0.3$) does not scale due to the appearance of \emph{stragglers}. 

Different factors like opening the trajectory file, or other sources of overheads can be responsible for observing \emph{stragglers} for I/O bound workload.
But, for the I/O bound workload, both communication and I/O appeared to be the main scalability bottlenecks when using a shared file.
Our data suggest that \emph{stragglers are due to the competition between MPI and the Lustre file system on the shared Infini-band interconnect}.  
Since I/O traffic must compete with MPI messages and other traffics, it can be difficult to achieve peak single-node I/O rates for an arbitrary file when the system is fully loaded with other jobs \cite{VMD2013, Kevin2018}. 

This is because when we remove I/O, communication does not appear to be the scalability bottleneck anymore (data not shown here).
In fact, communication time, \tcomm, could take
much longer for \emph{stragglers} than for ``normal'' MPI ranks when I/O has to be performed through a shared trajectory file (Figure \ref{fig:MPIranks}). 

Additionally, the number of I/O requests is a function of number of frames in the trajectory. 
For I/O bound task and compute bound task with the same number of frames per trajectory, the frequency of sending the I/O requests makes a big difference.
For sufficiently large per-frame compute workload, the I/O requests interfere much less often with communication than an I/O bound task.
This is why both communication and I/O appeared to prevent us from achieving the near ideal scaling for an I/O bound task.

It should be also noted that, for $\frac{\tcomp}{\tcomm} \gg 1$ and $\frac{\tcomp}{\tIO} \gg 1$, the effect of communication was less pronounced when the work became more compute-bound and this 
is because with compute-bound tasks there is less competition over accessing the shared trajectory file.
We showed this effect by changing the ratio between compute load and I/O load and studying its impact on the performance.

Therefore, for I/O bound tasks we needed to come up with solution to overcome \emph{stragglers}. 
We were able to achieve much better performance in our RMSD benchmark when we used global array toolkit instead of message-passing interface for communication. 
Using global array, we did not observe any delayed task due to communication (Figure \ref{fig:MPIwithIO-ga4py}) and it significantly reduced the communication cost. 
However, reducing communication cost was not enough for achieving near ideal scaling because I/O remains a bottleneck.

We showed several approaches to improve I/O scaling.
We were able to improve I/O through splitting the shared trajectory file (Figure \ref{fig:MPIwithIO-split}) and MPI-based parallel I/O through HDF5 file (Figure \ref{fig:MPIwithIO-hdf5}). 
In both cases, we were able to achieve near ideal scaling.
With splitting the trajectories, effect of communication is still apparent on the performance which is because $\frac{\tcomp}{\tcomm} \ll 1$; however together with 
global array toolkit we could achieve near ideal scaling (Figure \ref{fig:MPIwithIO-split}).
\obnote{I actually do not understand why we need GA for splitting but not for parallel MPI.}
\mknote{I mentioned before in my presentation in Spidal meeting that I myself do not have an answer for this.}

All the above strategies, provide guidelines for parallel analyses on data generated from MD simulations in \package{MDAnalysis} library.
The analysis indicates that splitting the trajectories in combination with global array or parallel I/O will make it feasible to run an I/O bound task on scalable computers up to 8 nodes and achieve near ideal scaling behavior.
In addition, we have examined all these benchmarks on several HPC resources in order to show the robustness of our approach.