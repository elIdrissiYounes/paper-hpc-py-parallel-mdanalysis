%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Oliver Beckstein at 2019-01-23 02:46:39 -0700 


%% Saved with string encoding Unicode (UTF-8) 



@article{Buyl:2014aa,
	Abstract = {We propose a new file format named ``H5MD'' for storing molecular simulation data, such as trajectories of particle positions and velocities, along with thermodynamic observables that are monitored during the course of the simulation. H5MD files are HDF5 (Hierarchical Data Format) files with a specific hierarchy and naming scheme. Thus, H5MD inherits many benefits of HDF5, e.g., structured layout of multi-dimensional datasets, data compression, fast and parallel I/O, and portability across many programming languages and hardware platforms. H5MD files are self-contained, and foster the reproducibility of scientific data and the interchange of data between researchers using different simulation programs and analysis software. In addition, the H5MD specification can serve for other kinds of data (e.g. experimental data) and is extensible to supplemental data, or may be part of an enclosing file structure.},
	Author = {Pierre de Buyl and Peter H. Colberg and Felix H{\"o}fling},
	Date-Added = {2019-01-23 02:46:35 -0700},
	Date-Modified = {2019-01-23 02:46:35 -0700},
	Doi = {10.1016/j.cpc.2014.01.018},
	Issn = {0010-4655},
	Journal = {Computer Physics Communications},
	Keywords = {Molecular simulation, HDF5, H5MD, file format},
	Number = {6},
	Pages = {1546 - 1553},
	Title = {{H5MD}: A structured, efficient, and portable file format for molecular data},
	Volume = {185},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0010465514000447},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.cpc.2014.01.018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBkLi4vLi4vLi4vLi4vLi4vRG9jdW1lbnRzL1B1YmxpY2F0aW9ucy9wZGYvQmliRGVzay9Db21wdXRlciBQaHlzaWNzIENvbW11bmljYXRpb25zXzE4NV9CdXlsXzIwMTRhLnBkZk8RAhoAAAAAAhoAAgAABERhdGEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANMkDAxIKwAAABYyQB9Db21wdXRlciBQaHlzaWNzIENvbSM4MjAxNjkucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAggFp2G2FfQAAAAAAAAAAAAUABQAACQAAAAAAAAAAAAAAAAAAAAAHQmliRGVzawAAEAAIAADTJG58AAAAEQAIAADYbeftAAAAAQAUABYyQAAWMj4AFi9YABU+DAAAGeEAAgBTRGF0YTpvbGl2ZXI6AERvY3VtZW50czoAUHVibGljYXRpb25zOgBwZGY6AEJpYkRlc2s6AENvbXB1dGVyIFBoeXNpY3MgQ29tIzgyMDE2OS5wZGYAAA4AZgAyAEMAbwBtAHAAdQB0AGUAcgAgAFAAaAB5AHMAaQBjAHMAIABDAG8AbQBtAHUAbgBpAGMAYQB0AGkAbwBuAHMAXwAxADgANQBfAEIAdQB5AGwAXwAyADAAMQA0AGEALgBwAGQAZgAPAAoABABEAGEAdABhABIAXS9vbGl2ZXIvRG9jdW1lbnRzL1B1YmxpY2F0aW9ucy9wZGYvQmliRGVzay9Db21wdXRlciBQaHlzaWNzIENvbW11bmljYXRpb25zXzE4NV9CdXlsXzIwMTRhLnBkZgAAEwANL1ZvbHVtZXMvRGF0YQD//wAAAAgADQAaACQAiwAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAKp}}

@url{POSIX2017,
	Date-Added = {2019-01-22 14:50:36 -0700},
	Date-Modified = {2019-01-22 14:52:53 -0700},
	Title = {https://www.nextplatform.com/2017/09/11/whats-bad-posix-io/, What is So Bad About POSIX I/O?},
	Urldate = {2017}}

@inproceedings{ATPESC2016,
	Author = {Robert Latham and Phil Carns},
	Booktitle = {ATPESC},
	Date-Added = {2019-01-22 21:41:18 +0000},
	Date-Modified = {2019-01-22 21:49:32 +0000},
	Title = {Thinking about HPC IO and HPC storage},
	Year = {2016}}

@inproceedings{Brown:2018ab,
	Acmid = {3225144},
	Address = {New York, NY, USA},
	Articleno = {7},
	Author = {Brown, Kevin A. and Jain, Nikhil and Matsuoka, Satoshi and Schulz, Martin and Bhatele, Abhinav},
	Booktitle = {Proceedings of the 47th International Conference on Parallel Processing},
	Date-Added = {2019-01-21 02:13:35 -0700},
	Date-Modified = {2019-01-21 02:13:35 -0700},
	Doi = {10.1145/3225058.3225144},
	Isbn = {978-1-4503-6510-9},
	Keywords = {I/O, Interference, MPI, fat-tree, network, simulation, HPC},
	Location = {Eugene, OR, USA},
	Numpages = {10},
	Pages = {7:1--7:10},
	Publisher = {ACM},
	Series = {ICPP 2018},
	Title = {Interference Between {I/O} and {MPI} Traffic on Fat-tree Networks},
	Url = {http://doi.acm.org/10.1145/3225058.3225144},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBGLi4vLi4vLi4vLi4vLi4vRG9jdW1lbnRzL1B1YmxpY2F0aW9ucy9wZGYvQmliRGVzay9JQ1BQX0Jyb3duXzIwMThhLnBkZk8RAbQAAAAAAbQAAgAABERhdGEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANMkDAxIKwAAABYyQBRJQ1BQX0Jyb3duXzIwMThhLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgUPo2Dfg7wAAAAAAAAAAAAUABQAACQAAAAAAAAAAAAAAAAAAAAAHQmliRGVzawAAEAAIAADTJG58AAAAEQAIAADYOENfAAAAAQAUABYyQAAWMj4AFi9YABU+DAAAGeEAAgBIRGF0YTpvbGl2ZXI6AERvY3VtZW50czoAUHVibGljYXRpb25zOgBwZGY6AEJpYkRlc2s6AElDUFBfQnJvd25fMjAxOGEucGRmAA4AKgAUAEkAQwBQAFAAXwBCAHIAbwB3AG4AXwAyADAAMQA4AGEALgBwAGQAZgAPAAoABABEAGEAdABhABIAPy9vbGl2ZXIvRG9jdW1lbnRzL1B1YmxpY2F0aW9ucy9wZGYvQmliRGVzay9JQ1BQX0Jyb3duXzIwMThhLnBkZgAAEwANL1ZvbHVtZXMvRGF0YQD//wAAAAgADQAaACQAbQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAIl},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3225058.3225144},
	Bdsk-Url-2 = {https://doi.org/10.1145/3225058.3225144},
	Bdsk-Url-3 = {http://dx.doi.org/10.1145/3225058.3225144}}

@inproceedings{Howison:2010aa,
	Abstract = {HDF5 is a cross-platform parallel I/O library that is used by a wide variety of HPC applications for the flexibility of its hierarchical object-database representation of scientific data. We describe our recent work to optimize the performance of the HDF5 and MPI-IO libraries for the Lustre parallel file system. We selected three different HPC applications to represent the diverse range of I/O requirements, and measured their performance on three different systems to demonstrate the robustness of our optimizations across different file system configurations and to validate our optimization strategy. We demonstrate that the combined optimizations improve HDF5 parallel I/O performance by up to 33 times in some cases -- running close to the achievable peak performance of the underlying file system -- and demonstrate scalable performance up to 40,960-way concurrency.},
	Address = {Heraklion, Crete, Greece},
	Author = {Howison, Mark and Quincey Koziol and David Knaak and John Mainzer and John Shalf},
	Booktitle = {Proceedings of Workshop on Interfaces and Abstractions for Scientific Data Storage},
	Date-Added = {2018-12-12 14:32:53 -0700},
	Date-Modified = {2018-12-12 14:32:53 -0700},
	Keywords = {HDF5, Lustre, tuning, file systems},
	Month = {September},
	Pages = {LBNL-4803E},
	Title = {Tuning {HDF5} for {Lustre} file systems},
	Year = {2010}}

@inproceedings{Paraskevakos:2018aa,
	Abstract = {Different frameworks for implementing parallel data analytics applications have been proposed by the HPC and Big Data communities. In this paper, we investigate three frame- works: Spark, Dask and RADICAL-Pilot with respect to their ability to support data analytics requirements on HPC resources. We investigate the data analysis requirements of Molecular Dy- namics (MD) simulations which are significant consumers of su- percomputing cycles, producing immense amounts of data: a typ- ical large-scale MD simulation of physical systems of O(100,000) atoms can produce from O(10) GB to O(1000) GBs of data. We propose and evaluate different approaches for parallelization of a representative set of MD trajectory analysis algorithms, in partic- ular the computation of path similarity and the identification of connected atom. We evaluate Spark, Dask and RADICAL-Pilot with respect to the provided abstractions and runtime engine ca- pabilities to support these algorithms. We provide a conceptual basis for comparing and understanding the different frameworks that enable users to select the optimal system for its application. Further, we provide a quantitative performance analysis of the different algorithms across the three frameworks using different high-performance computing resources.},
	Address = {New York, NY, USA},
	Author = {Ioannis Paraskevakos and Andre Luckow and Mahzad Khoshlessan and Goerge Chantzialexiou and Thomas E. Cheatham and Oliver Beckstein and Geoffrey Fox and Shantenu Jha},
	Booktitle = {ICPP 2018: 47th International Conference on Parallel Processing, August 13--16, 2018, Eugene, OR, USA},
	Date-Added = {2018-12-12 14:22:51 -0700},
	Date-Modified = {2018-12-12 14:22:51 -0700},
	Doi = {10.1145/3225058.3225128},
	Keywords = {Data analytics, MD Simulations Analysis, Parallel , MDAnalysis, task-parallel, PSA, LeafletFinder},
	Month = {August 13--16},
	Organization = {Association for Computing Machinery},
	Publisher = {ACM},
	Title = {Task-parallel Analysis of Molecular Dynamics Trajectories},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBNLi4vLi4vLi4vLi4vLi4vRG9jdW1lbnRzL1B1YmxpY2F0aW9ucy9wZGYvQmliRGVzay9JQ1BQX1BhcmFza2V2YWtvc18yMDE4YS5wZGZPEQHQAAAAAAHQAAIAAAREYXRhAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTJAwMSCsAAAAWMkAbSUNQUF9QYXJhc2tldmFrb3NfMjAxOGEucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHVAkNbs4U4AAAAAAAAAAAAFAAUAAAkAAAAAAAAAAAAAAAAAAAAAB0JpYkRlc2sAABAACAAA0yRufAAAABEACAAA1u1DvgAAAAEAFAAWMkAAFjI+ABYvWAAVPgwAABnhAAIAT0RhdGE6b2xpdmVyOgBEb2N1bWVudHM6AFB1YmxpY2F0aW9uczoAcGRmOgBCaWJEZXNrOgBJQ1BQX1BhcmFza2V2YWtvc18yMDE4YS5wZGYAAA4AOAAbAEkAQwBQAFAAXwBQAGEAcgBhAHMAawBlAHYAYQBrAG8AcwBfADIAMAAxADgAYQAuAHAAZABmAA8ACgAEAEQAYQB0AGEAEgBGL29saXZlci9Eb2N1bWVudHMvUHVibGljYXRpb25zL3BkZi9CaWJEZXNrL0lDUFBfUGFyYXNrZXZha29zXzIwMThhLnBkZgATAA0vVm9sdW1lcy9EYXRhAP//AAAACAANABoAJAB0AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAkg=},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/3225058.3225128}}

@article{Mache:2005aa,
	Abstract = {Network contention hotspots can limit network throughput for parallel disk I/O, even when the interconnection network appears to be sufficiently provisioned. We studied I/O hotspots in mesh networks as a function of the spatial layout of an application's compute nodes relative to the I/O nodes. Our analytical modeling and dynamic simulations show that when I/O nodes are configured on one side of a two-dimensional mesh, realizable I/O throughput is at best bounded by four times the network bandwidth per link. Maximal performance depends on the spatial layout of jobs, and cannot be further improved by adding I/O nodes. Applying these results, we devised a new parallel layout allocation strategy (PLAS) which minimizes I/O hotspots, and approaches the theoretical best case for parallel I/O throughput. Our I/O performance analysis and processor allocation strategy are applicable to a wide range of contemporary and emerging high-performance computing systems.},
	Author = {Jens Mache and Virginia Lo and Sharad Garg},
	Date-Added = {2018-12-12 14:21:33 -0700},
	Date-Modified = {2018-12-12 14:21:33 -0700},
	Doi = {10.1016/j.jpdc.2005.04.020},
	Issn = {0743-7315},
	Journal = {Journal of Parallel and Distributed Computing},
	Keywords = {Mesh interconnection networks, Network contention, Parallel I/O, Job scheduling, Processor allocation},
	Note = {Design and Performance of Networks for Super-, Cluster-, and Grid-Computing Part I},
	Number = {10},
	Pages = {1190 - 1203},
	Title = {The impact of spatial layout of jobs on {I/O} hotspots in mesh networks},
	Url = {http://www.sciencedirect.com/science/article/pii/S0743731505001048},
	Volume = {65},
	Year = {2005},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0743731505001048},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jpdc.2005.04.020},
	Bdsk-Url-3 = {http://dx.doi.org/10.1016/j.jpdc.2005.04.020}}

@article{scalable-IO1,
	Author = {Seung Woo Son and Saba Sehrish and Wei-keng Liao and Ron Oldfield and Alok Choudhary},
	Date-Added = {2018-12-05 16:34:07 -0700},
	Date-Modified = {2018-12-05 16:43:06 -0700},
	Journal = {Journal of Supercomputing},
	Number = {5},
	Pages = {pp 2069--2097},
	Title = {Reducing I/O Variability using Dynamic I/O Path Characterization in Petascale Storage Systems},
	Volume = {73},
	Year = {2017}}

@article{scalable-IO,
	Author = {Alok Choudhary and Wei-keng Liao and Kui Gao and Arifa Nisar and Robert Ross and Rajeev Thakur and Robert Latham},
	Date-Added = {2018-12-05 16:30:08 -0700},
	Date-Modified = {2018-12-05 16:32:56 -0700},
	Journal = {Journal of Physics: Conference Series},
	Number = {012048},
	Title = {Scalable I/O and analytics},
	Volume = {180},
	Year = {2009}}

@inproceedings{optimize_lustre,
	Author = {Kuan-Wu Lin and Jerry Chou and Surendra Byna and Kesheng Wu and},
	Booktitle = {SSDBM Proceedings of the 25th International Conference on Scientific and Statistical Database Management},
	Date-Added = {2018-09-16 15:16:34 -0700},
	Date-Modified = {2018-12-12 11:35:34 -0700},
	Month = {July 29 - 31},
	Number = {Article No. 29},
	Title = {Optimizing Fast query Performance on {Lustre} File System},
	Year = {2013}}

@inproceedings{Nguyen2017,
	Annote = {not needed},
	Author = {Nguyen, Nhan and Khan, Mohammad Maifi Hasan and Albayram, Yusuf and Wang, Kewen},
	Booktitle = {2017 IEEE 10th International Conference on Cloud Computing (CLOUD)},
	Doi = {10.1109/CLOUD.2017.119},
	File = {:D$\backslash$:/Mendeley/Nguyen et al/2017 IEEE 10th International Conference on Cloud Computing (CLOUD)/08030677.pdf:pdf},
	Isbn = {978-1-5386-1993-3},
	Month = {jun},
	Pages = {802--807},
	Publisher = {IEEE},
	Title = {{Understanding the Influence of Configuration Settings: An Execution Model-Driven Framework for Apache Spark Platform}},
	Url = {http://ieeexplore.ieee.org/document/8030677/},
	Year = {2017},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/document/8030677/},
	Bdsk-Url-2 = {https://doi.org/10.1109/CLOUD.2017.119},
	Bdsk-Url-3 = {http://dx.doi.org/10.1109/CLOUD.2017.119}}

@inproceedings{Yang2016,
	Author = {Yang, Hongbin and Liu, Xianyang and Chen, Shenbo and Lei, Zhou and Du, Hongguang and Zhu, Caixin},
	Booktitle = {2016 International Conference on Audio, Language and Image Processing (ICALIP)},
	Doi = {10.1109/ICALIP.2016.7846627},
	File = {:D$\backslash$:/Mendeley/Yang et al/2016 International Conference on Audio, Language and Image Processing (ICALIP)/07846627.pdf:pdf},
	Isbn = {978-1-5090-0654-0},
	Month = {jul},
	Pages = {28--33},
	Publisher = {IEEE},
	Title = {{Improving Spark performance with MPTE in heterogeneous environments}},
	Url = {http://ieeexplore.ieee.org/document/7846627/},
	Year = {2016},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/document/7846627/},
	Bdsk-Url-2 = {https://doi.org/10.1109/ICALIP.2016.7846627},
	Bdsk-Url-3 = {http://dx.doi.org/10.1109/ICALIP.2016.7846627}}

@article{Ousterhout2017,
	Author = {Ousterhout, Kay},
	File = {:D$\backslash$:/Mendeley/Ousterhout/Unknown/Ousterhout - 2017 - Architecting for Performance Clarity in Data Analytics Frameworks.pdf:pdf},
	Title = {{Architecting for Performance Clarity in Data Analytics Frameworks}},
	Url = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-158.html},
	Year = {2017},
	Bdsk-Url-1 = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-158.html}}

@inproceedings{Wang2014,
	Address = {New York, New York, USA},
	Author = {Wang, Da and Joshi, Gauri and Wornell, Gregory and Wang, Da and Joshi, Gauri and Wornell, Gregory},
	Booktitle = {The 2014 ACM international conference on Measurement and modeling of computer systems - SIGMETRICS '14},
	Doi = {10.1145/2591971.2592042},
	File = {:D$\backslash$:/Mendeley/Wang et al/The 2014 ACM international conference on Measurement and modeling of computer systems - SIGMETRICS '14/Wang et al. - 2014 - Efficient task replication for fast response times in parallel computation.pdf:pdf},
	Isbn = {9781450327893},
	Issn = {0163-5999},
	Keywords = {backup tasks,latency,parallel computation,redundancy,scheduling,task replication,variability},
	Number = {1},
	Pages = {599--600},
	Publisher = {ACM Press},
	Title = {{Efficient task replication for fast response times in parallel computation}},
	Url = {http://dl.acm.org/citation.cfm?doid=2591971.2592042},
	Volume = {42},
	Year = {2014},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?doid=2591971.2592042},
	Bdsk-Url-2 = {https://doi.org/10.1145/2591971.2592042},
	Bdsk-Url-3 = {http://dx.doi.org/10.1145/2591971.2592042}}

@article{Harlap2015,
	Abstract = {Parallel executions of iterative machine learning (ML) algorithms can suffer significant performance losses to stragglers. The regular (e.g., per iteration) barriers used in the traditional BSP approach cause every transient slowdown of any worker thread to delay all others. This paper describes a scalable, efficient solution to the straggler problem for this important class of parallel ML problems, combining a more flexible synchronization model with dynamic peer-to-peer re-assignment of work among workers. Experiments with both synthetic straggler behaviors and real straggler behavior observed on Amazon EC2 confirm the significance of the problem and the effectiveness of the solution, as implemented in a framework called FlexRR. Using FlexRR, we consistently observe near-ideal run-times (relative to no performance jitter) across all straggler patterns tested. Acknowledgements: We thank Jim Cipar, Qirong Ho, Jin Kyu Kim, Senghyeuk Lee, and other big-learning collaborators for our many insightful discussions about system support for large-scale ML. We thank Ron Brightwell, Kurt Ferreira, Kevin Pedretti, and Lee Ward of Sandia National Laboratories for their insight about OS Jitters. We thank the members and companies of the PDL Consortium (including Actifio, APC, EMC, Emulex, Facebook, Fusion-IO, Google, Hewlett-Packard, Hitachi, Huawei, Intel, Microsoft, NEC Labs, NetApp, Oracle, Panasas, Riverbed, Samsung, Seagate, STEC, Symantec, VMWare, Western Digital) for their interest, insights, feedback, and support.},
	Author = {Harlap, Aaron and Cui, Henggang and Dai, Wei and Wei, Jinliang and Ganger, Gregory R and Gibbons, Phillip B and Gibson, Garth A and Xing, Eric P},
	File = {:D$\backslash$:/Mendeley/Harlap et al/Unknown/Harlap et al. - 2015 - Solving the straggler problem for iterative convergent parallel ML.pdf:pdf},
	Title = {{Solving the straggler problem for iterative convergent parallel ML}},
	Url = {http://www.pdl.cmu.edu/PDL-FTP/BigLearning/CMU-PDL-15-102.pdf},
	Year = {2015},
	Bdsk-Url-1 = {http://www.pdl.cmu.edu/PDL-FTP/BigLearning/CMU-PDL-15-102.pdf}}

@article{Daily2010,
	Abstract = {---Global Arrays (GA) is a software system from Pacific Northwest National Laboratory that enables an efficient, portable, and parallel shared-memory programming interface to manipulate distributed dense arrays. Using a combination of GA and NumPy, we have reimplemented NumPy as a distributed drop-in replacement called Global Arrays in NumPy (GAiN). Scalability studies will be presented showing the utility of developing serial NumPy codes which can later run on more capable clusters or supercomputers.},
	Author = {Daily, Jeff and Lewis, Robert R},
	File = {:D$\backslash$:/Mendeley/Daily, Lewis/PROC. OF THE 9th PYTHON IN SCIENCE CONF/Daily, Lewis - 2010 - Using the Global Arrays Toolkit to Reimplement NumPy for Distributed Computation.pdf:pdf},
	Journal = {PROC. OF THE 9th PYTHON IN SCIENCE CONF},
	Keywords = {Index Terms---Global Arrays,MPI,NumPy,Python},
	Title = {{Using the Global Arrays Toolkit to Reimplement NumPy for Distributed Computation}},
	Url = {http://hpc.pnl.gov/globalarrays/papers/scipy11{\_}gain.pdf},
	Year = {2010},
	Bdsk-Url-1 = {http://hpc.pnl.gov/globalarrays/papers/scipy11%7B%5C_%7Dgain.pdf}}

@inproceedings{Marcu2016,
	Author = {Marcu, Ovidiu-Cristian and Costan, Alexandru and Antoniu, Gabriel and Perez-Hernandez, Maria S.},
	Booktitle = {2016 IEEE International Conference on Cluster Computing (CLUSTER)},
	Doi = {10.1109/CLUSTER.2016.22},
	File = {:D$\backslash$:/Mendeley/Marcu et al/2016 IEEE International Conference on Cluster Computing (CLUSTER)/07776539.pdf:pdf},
	Isbn = {978-1-5090-3653-0},
	Month = {sep},
	Pages = {433--442},
	Publisher = {IEEE},
	Title = {{Spark Versus Flink: Understanding Performance in Big Data Analytics Frameworks}},
	Url = {http://ieeexplore.ieee.org/document/7776539/},
	Year = {2016},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/document/7776539/},
	Bdsk-Url-2 = {https://doi.org/10.1109/CLUSTER.2016.22},
	Bdsk-Url-3 = {http://dx.doi.org/10.1109/CLUSTER.2016.22}}

@article{Garraghan2016,
	Annote = {Print},
	Author = {Garraghan, Peter and Ouyang, Xue and Yang, Renyu and McKee, David and Xu, Jie},
	Doi = {10.1109/TSC.2016.2611578},
	File = {:D$\backslash$:/Mendeley/Garraghan et al/IEEE Transactions on Services Computing/07572191.pdf:pdf},
	Issn = {1939-1374},
	Journal = {IEEE Transactions on Services Computing},
	Pages = {1--1},
	Title = {{Straggler Root-Cause and Impact Analysis for Massive-scale Virtualized Cloud Datacenters}},
	Url = {http://ieeexplore.ieee.org/document/7572191/},
	Year = {2016},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/document/7572191/},
	Bdsk-Url-2 = {https://doi.org/10.1109/TSC.2016.2611578},
	Bdsk-Url-3 = {http://dx.doi.org/10.1109/TSC.2016.2611578}}

@inproceedings{Li2016,
	Author = {Li, Cong and Shen, Huanxing and Huang, Tai},
	Booktitle = {2016 9th Workshop on Many-Task Computing on Clouds, Grids, and Supercomputers (MTAGS)},
	Doi = {10.1109/MTAGS.2016.04},
	File = {:D$\backslash$:/Mendeley/Li, Shen, Huang/2016 9th Workshop on Many-Task Computing on Clouds, Grids, and Supercomputers (MTAGS)/07836548.pdf:pdf},
	Isbn = {978-1-5090-5212-7},
	Month = {nov},
	Pages = {1--6},
	Publisher = {IEEE},
	Title = {{Learning to Diagnose Stragglers in Distributed Computing}},
	Url = {http://ieeexplore.ieee.org/document/7836548/},
	Year = {2016},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/document/7836548/},
	Bdsk-Url-2 = {https://doi.org/10.1109/MTAGS.2016.04},
	Bdsk-Url-3 = {http://dx.doi.org/10.1109/MTAGS.2016.04}}

@inproceedings{Kyong2017,
	Address = {New York, USA},
	Author = {Kyong, Joohyun and Jeon, Jinwoo and Lim, Sung-Soo},
	Booktitle = {Proceedings of the 6th International Conference on Software and Computer Applications - ICSCA '17},
	Date-Modified = {2018-09-30 21:10:48 -0700},
	Doi = {10.1145/3056662.3056686},
	File = {:D$\backslash$:/Mendeley/Kyong, Jeon, Lim/Proceedings of the 6th International Conference on Software and Computer Applications - ICSCA '17/Kyong, Jeon, Lim - 2017 - Improving scalability of apache spark-based scale-up server through docker container-based partit.pdf:pdf},
	Isbn = {9781450348577},
	Keywords = {apache spark,docker,salability,scale-up},
	Pages = {176--180},
	Publisher = {ACM Press},
	Title = {{Improving scalability of apache spark-based scale-up server through docker container-based partitioning}},
	Url = {http://dl.acm.org/citation.cfm?doid=3056662.3056686},
	Year = {2017},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?doid=3056662.3056686},
	Bdsk-Url-2 = {https://doi.org/10.1145/3056662.3056686},
	Bdsk-Url-3 = {http://dx.doi.org/10.1145/3056662.3056686}}

@inproceedings{Gittens2016,
	Author = {Gittens, Alex and Devarakonda, Aditya and Racah, Evan and Ringenburg, Michael and Gerhardt, Lisa and Kottalam, Jey and Liu, Jialin and Maschhoff, Kristyn and Canon, Shane and Chhugani, Jatin and Sharma, Pramod and Yang, Jiyan and Demmel, James and Harrell, Jim and Krishnamurthy, Venkat and Mahoney, Michael W. and Prabhat},
	Booktitle = {IEEE International Conference on Big Data (Big Data)},
	Date-Modified = {2018-09-30 21:13:05 -0700},
	Doi = {10.1109/BigData.2016.7840606},
	File = {:D$\backslash$:/Mendeley/Gittens et al/2016 IEEE International Conference on Big Data (Big Data)/07840606.pdf:pdf},
	Isbn = {978-1-4673-9005-7},
	Month = {dec},
	Pages = {204--213},
	Title = {Matrix factorizations at scale: A comparison of scientific data analytics in spark and C+MPI using three case studies},
	Url = {http://ieeexplore.ieee.org/document/7840606/},
	Year = {2016},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/document/7840606/},
	Bdsk-Url-2 = {https://doi.org/10.1109/BigData.2016.7840606},
	Bdsk-Url-3 = {http://dx.doi.org/10.1109/BigData.2016.7840606}}

@inproceedings{mahzad2017scipy,
	Abstract = {The analysis of biomolecular computer simulations has become a challenge because the amount of output data is now routinely in the terabyte range. We evaluated if this challenge can be met by a parallel map-reduce approach with the Dask parallel computing library for task-graph based com- puting coupled with our MDAnalysis Python library for the analysis of molecular dynamics (MD) simulations. We performed a representative performance evalu- ation, taking into account the highly heterogeneous computing environment that researchers typically work in together with the diversity of existing file formats for MD trajectory data. We found that the underlying storage system (solid state drives, parallel file systems, or simple spinning platter disks) can be a deciding performance factor that leads to data ingestion becoming the primary bottleneck in the analysis work flow. However, the choice of the data file format can mitigate the effect of the storage system; in particular, the commonly used Gromacs XTC trajectory format, which is highly compressed, can exhibit strong scaling close to ideal due to trading a decrease in global storage access load against an increase in local per-core CPU-intensive decompression. Scaling was tested on a single node and multiple nodes on national and local supercomputing resources as well as typical workstations. Although very good strong scaling could be achieved for single nodes, good scaling across multiple nodes was hindered by the persistent occurrence of "stragglers", tasks that take much longer than all other tasks, and whose ultimate cause could not be completely ascertained. In summary, we show that, due to the focus on high interoperability in the scientific Python eco system, it is straightforward to implement map-reduce with Dask in MDAnalysis and provide an in-depth analysis of the considerations to obtain good parallel performance on HPC resources.},
	Address = {Austin, TX},
	Author = {Khoshlessan, Mahzad and Paraskevakos, Ioannis and Jha, Shantenu and Beckstein, Oliver},
	Booktitle = {{P}roceedings of the 16th {P}ython in {S}cience {C}onference},
	Date-Added = {2017-06-13 01:01:54 +0000},
	Date-Modified = {2017-10-14 11:19:24 +0000},
	Doi = {10.25080/shinma-7f4c6e7-00a},
	Editor = {{K}aty {H}uff and {D}avid {L}ippa and {D}illon {N}iederhut and {M} {P}acer},
	Keywords = {MDAnalysis, High Performance Computing, Dask, Map-Reduce, MPI for Python},
	Organization = {SciPy},
	Pages = {64--72},
	Title = {Parallel Analysis in {MDAnalysis} using the {Dask} Parallel Computing Library},
	Year = 2017,
	Bdsk-Url-1 = {https://doi.org/10.25080/shinma-7f4c6e7-00a},
	Bdsk-Url-2 = {http://dx.doi.org/10.25080/shinma-7f4c6e7-00a}}


@inproceedings{Daily:2014aa,
	Abstract = {Partitioned Global Address Space (PGAS) models are emerging as a popular alternative to MPI models for designing scalable applications. At the same time, MPI remains a ubiquitous communication subsystem due to its standardization, high performance, and availability on leading platforms. In this paper, we explore the suitability of using MPI as a scalable PGAS communication subsystem. We focus on the Remote Memory Access (RMA) communication in PGAS models which typically includes get, put, and atomic memory operations. We perform an in-depth exploration of design alternatives based on MPI. These alternatives include using a semantically-matching interface such as MPI-RMA, as well as not-so-intuitive interfaces such as MPI two-sided with a combination of multi-threading and dynamic process management. With an in-depth exploration of these alternatives and their shortcomings, we propose a novel design which is facilitated by the data-centric view in PGAS models. This design leverages a combination of highly tuned MPI two-sided semantics and an automatic, user-transparent split of MPI communicators to provide asynchronous progress. We implement the asynchronous progress ranks approach and other approaches within the Communication Runtime for Exascale which is a communication subsystem for Global Arrays. Our performance evaluation spans pure communication benchmarks, graph community detection and sparse matrix-vector multiplication kernels, and a computational chemistry application. The utility of our proposed PR-based approach is demonstrated by a 2.17x speedup on 1008 processors over the other MPI-based designs.},
	Author = {J. Daily and A. Vishnu and B. Palmer and H. van Dam and D. Kerbyson},
	Booktitle = {2014 21st International Conference on High Performance Computing (HiPC)},
	Date-Added = {2019-02-07 09:06:51 -0700},
	Date-Modified = {2019-02-07 09:07:24 -0700},
	Doi = {10.1109/HiPC.2014.7116712},
	Issn = {1094-7256},
	Keywords = {chemistry computing;graph theory;matrix multiplication;message passing;multi-threading;sparse matrices;PGAS runtime;partitioned global address space models;scalable applications;standardization;scalable PGAS communication subsystem;remote memory access communication;RMA;atomic memory operations;semantically-matching interface;MPI-RMA;not-so-intuitive interfaces;multithreading;dynamic process management;user-transparent split;global arrays;pure communication benchmarks;graph community detection;sparse matrix-vector multiplication kernels;computational chemistry application;MPI-based designs;Electronics packaging;Semantics;Protocols;Message systems;Runtime;Synchronization;Computational modeling; global arrays; MPI},
	Month = {Dec},
	Pages = {1-10},
	Title = {On the suitability of {MPI} as a {PGAS} runtime},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/HiPC.2014.7116712},
	}


@inproceedings{Xie:2012aa,
	Abstract = {Supercomputer I/O loads are often dominated by writes. HPC (High Performance Computing) file systems are designed to absorb these bursty outputs at high bandwidth through massive parallelism. However, the delivered write bandwidth often falls well below the peak. This paper characterizes the data absorption behavior of a center-wide shared Lustre parallel file system on the Jaguar supercomputer. We use a statistical methodology to address the challenges of accurately measuring a shared machine under production load and to obtain the distribution of bandwidth across samples of compute nodes, storage targets, and time intervals. We observe and quantify limitations from competing traffic, contention on storage servers and I/O routers, concurrency limitations in the client compute node operating systems, and the impact of variance (stragglers) on coupled output such as striping. We then examine the implications of our results for application performance and the design of I/O middleware systems on shared supercomputers.},
	Acmid = {2389007},
	Address = {Los Alamitos, CA, USA},
	Articleno = {8},
	Author = {Xie, Bing and Chase, Jeffrey and Dillow, David and Drokin, Oleg and Klasky, Scott and Oral, Sarp and Podhorszki, Norbert},
	Booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
	Date-Added = {2018-12-13 13:17:42 -0700},
	Date-Modified = {2018-12-13 13:18:53 -0700},
	Isbn = {978-1-4673-0804-5},
	Keywords = {I/O bottleneck, writing, HPC, stragglers},
	Location = {Salt Lake City, Utah},
	Numpages = {11},
	Pages = {8:1--8:11},
	Publisher = {IEEE Computer Society Press},
	Series = {SC '12},
	Title = {Characterizing Output Bottlenecks in a Supercomputer},
	Url = {http://dl.acm.org/citation.cfm?id=2388996.2389007},
	Year = {2012},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2388996.2389007}}
