\begin{abstract} 
Typical size of the molecular dynamics (MD) trajectories from data-intensive bio-molecular simulations ranges from gigabytes to terabytes. 
However, the computational biophysics community misses effective use of high performance computing (HPC) resources for efficiently analyzing these trajectories and more importantly achieving linear scaling still remains a big challenge. 
\obnote{We will have to be a bit more careful how we phrase this because VMD can analyze on thousands of cores and cpptraj is apparently also doing pretty well. There is also HiMach.Benchmarks, however, are scarce.}
\mknote{Need to make clear how this work distinguishes from their work}
Present work aims to provide insights, guidelines and strategies to the community on how to take advantage of the available HPC resources to gain the best possible performance.
We investigated a single program multiple data (SPMD) execution model where each process executes the same program, to parallelize the Map-Reduce Root Mean Square Distance (RMSD) and Dihedral Featurization algorithms for analysis of MD trajectories in the MDAnalysis library. \gpnote{I am not sure RMSD can be stated as a MR algorithm. We do not employ any reduce
function to provide a single answer for the whole dataset. Am I missing something? }\mknote{No, but at the end we are gathering the data from all ranks to rank 0. Oliver believed that this can be considered as a MR too!}
We employ the Python language because it is widely used in the bio-molecular simulation field and focus on an MPI-based implementations.  
We notice that straggler tasks negatively impact the performance and act as scalability bottlenecks. 
Straggler tasks are a very common problem in heterogeneous environments and are significantly slower than the mean execution time of all tasks, impeding job completion time.
Our initial analysis shows that accessing a single file on the distributed file system leads to stragglers, and as a result, prevents any scaling beyond one node.
We introduce an important performance parameter $t_{Compute}$/$t_{IO}$ which determines whether we observe any stragglers.
In addition, we show that there are two factors that lead to stragglers including I/O and communication. 
Taking advantage of Global Arrays (GA) toolkit we have been able to obtain significant improvement in communication cost and performance.
In addition, we show two different approaches to overcome the I/O bottleneck and compare their performance. 
First approach is splitting the trajectory into as many trajectory segments as number of processes.
The second approach is through MPI-based approach using Parallel HDF5 where we examine the performance differences between both collective and independent I/O.
Applying these strategies, we obtained near ideal scaling and performance.  
\end{abstract}

%\keywords{Data analytics, MD Simulations Analysis, Parallel MD analysis, task-parallel}
