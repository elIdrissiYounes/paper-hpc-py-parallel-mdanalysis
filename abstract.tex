\begin{abstract} 
\package{MDAnalysis} is an open source Python data analysis library for post-processing the molecular dynamics (MD) trajectories and provides optimized classes and functions for important components of the scientific code. \package{MDAnalysis}, however, does not provide a full scalable parallel framework for analyzing these trajectories. 
Present work aims to provide insights, guidelines, and strategies to the community on how to take advantage of the available HPC resources to gain good performance inside \package{MDAnalysis}.
We investigated a single program multiple data (SPMD) execution model where each process executes the same program, to parallelize Root Mean Square Distance (RMSD) algorithm in the MDAnalysis library using a Map-Reduce approach. 
We employ the Python language because it is widely used in the bio-molecular simulation field and focus on MPI-based implementations.  
Straggler tasks are a very common problem in distributed environments and are significantly slower than the mean execution time of all tasks, impeding job completion time.
We notice that straggler tasks negatively impact the performance of the task and act as scalability bottlenecks.
Our initial analysis shows that accessing a single file on the distributed file system leads to stragglers, and as a result, prevents any scaling beyond a single node.
We introduce two important performance parameters $t_{\text{Compute}}$/$t_{\text{IO}}$ and $t_{\text{Compute}}$/$t_{\text{Communication}}$ which determines whether we observe any stragglers.
In addition, we show that I/O and communication lead to stragglers. 
Taking advantage of the Global Arrays (GA) toolkit we were able to obtain significant improvement in communication cost and performance.
In addition, we show two different approaches to overcome the I/O bottleneck and compare their performance. 
The first approach is splitting the trajectory into as many trajectory segments as number of processes.
The second approach is through MPI-based approach using Parallel HDF5 where we examine the performance through independent I/O.
Applying these strategies, we obtained near ideal scaling and performance up to 384 cores.  
\end{abstract}

