\begin{abstract} 
Typical size of the molecular dynamics (MD) trajectories from data-intensive bio-molecular simulations ranges from gigabytes to terabytes. 
Peta-scale molecular dynamics simulations provide a powerful tool for investigating biological systems.
The exponential growth in the computational power of these simulations has lead to these large output files.

\package{MDAnalysis} is an open source Python data analysis library for post-processing the MD output data and provides optimized classes and functions for important components of scientific code such as multidimensional arrays or linear algebra routines.
\package{MDAnalysis}; however, misses effective use of high performance computing (HPC) resources for efficiently analyzing these trajectories and more importantly achieving linear scaling still remains a big challenge. 

Present work aims to provide insights, guidelines and strategies to the community on how to take advantage of the available HPC resources to gain the best possible performance inside \package{MDAnalysis}.
We investigated a single program multiple data (SPMD) execution model where each process executes the same program, to parallelize the Map-Reduce Root Mean Square Distance (RMSD) and Dihedral Featurization algorithms for analysis of MD trajectories in the MDAnalysis library. 
We employ the Python language because it is widely used in the bio-molecular simulation field and focus on an MPI-based implementations.  
We notice that straggler tasks negatively impact the performance and act as scalability bottlenecks.
\gpnote{Okay, but we did not do any runs where Comet and SuperMic, for example, were used together. When I read about a hetergeneous environment, I expect the use of different systems concurrently for the same run. That is not the case with our experiments. The case of different machone is not satisfied. Furthermore, did you use in any of your runs any shared compute node? Because I did not and at least the condition of different task types in not satisfied. In a nutshell, as far as I know the experiments had no heterogeneity nor the system we used. This makes the following sentence misleading.}\mknote{Ok, makes sense. I used distributed instead}
Straggler tasks are a very common problem in distributed environments and are significantly slower ($\approx 6 \times$) than the mean execution time of all tasks, impeding job completion time.
Our initial analysis shows that accessing a single file on the distributed file system leads to stragglers, and as a result, prevents any scaling beyond a single node.
We introduce two important performance parameters $t_{Compute}$/$t_{IO}$ and $t_{Compute}$/$t_{Communication}$ which determines whether we observe any stragglers.

In addition, we show that I/O and communication lead to stragglers. 
Taking advantage of Global Arrays (GA) toolkit we have been able to obtain significant improvement in communication cost and performance.
In addition, we show two different approaches to overcome the I/O bottleneck and compare their performance. 
First approach is splitting the trajectory into as many trajectory segments as number of processes.
The second approach is through MPI-based approach using Parallel HDF5 where we examine the performance through independent I/O.
Applying these strategies, we obtained near ideal scaling and performance.  
\end{abstract}

%\keywords{Data analytics, MD Simulations Analysis, Parallel MD analysis, task-parallel}
