\label{background}

Parallel analysis frameworks are becoming very popular instead of the traditional sequential post-simulation data analysis methods as a result of increase in the computational power of MD simulations and size of the generated output dataset.

HiMach \cite{himach-2008} introduces scalable and flexible parallel Python framework to deal with massive MD trajectories, by combining and extending the strengths of Google\textsc{\char13}s MapReduce and VMD analysis tool. 
HiMach\textsc{\char13}s runtime is responsible to parallelize and distribute the Map and Reduce classes to the assigned cores.
HiMach uses parallel I/O for file access during the map tasks and \text{MPI\_Allgather} in the reduction process. 
HiMach, however, does not discuss parallel analysis for the task types which can not be analyzed using MapReduce implementation.
The analysis needs by MD community, however, are always varied and a generalized parallel framework should be able to provide parallel analysis for a variety of task types.

Wu et. al. \cite{Wu_et.al} present a scalable parallel framework for distributed-memory post-simulation data analysis.
This work consists of an interface that allows a user to write analysis programs sequentially, and the machinery that ensures these programs execute in parallel automatically. 
The main components of the proposed framework are (1) domain decomposition that splits computational domain into blocks with specified boundary conditions, (2) HDF5 based parallel I/O (3) data exchangethat communicates ghost atoms between neighbor blocks, and (4) parallel analysis implementation of a real-world analysis application.
Again, this work does not discuss parallel analysis for the task types which can not be analyzed using MapReduce implementation and is limited to HDF5 file format.

Zazen \cite{Zazen} is a novel task-assignment protocol to overcome the I/O bottleneck for many I/O bound tasks.This protocol caches a copy of simulation output files on the local disks of an analysis cluster\textsc{\char13}s compute nodes, and uses co-locate data access with computation. 
Zazen is implemented in a parallel disk cache system and avoids the overhead associated with querying metadata servers by reading data in parallel from local disks.
However, transferring Terabytes of data might not be feasible even if high performance data transfer tools are used for that purpose.

VMD \cite{VMD2013} provides molecular visualization and analysis tool through algorithmic and memory efficiency improvements, hand-vectorization of key CPU algorithms, new and improved GPU analysis andvisualization algorithms, and parallel I/O performance.
While VMD provides a very strong tool for use in the personal workstation or local clusters, it is not typically used for very heavy and large scale computations on HPC clusters.
In addition, VMD does not provide much flexibility for analysis of very specific task types and performing multiple types of analysis.

CPPTraj~\cite{cpptraj-2013} offers three levels of parallelization through a C++ implementation. The two are through MPI and the third through OpenMP.
CCPTraj allows the parallel read between frames of the same trajectory or ensemble members of the same trajectory. 
 pyPcazip \cite{pyPcazip} is a suite of software tools written in Python for compression and analysis of molecular dynamics (MD) simulation data. 
pyPcazip is MPI parallelised and is specific to PCA-based investigations of MD trajectories and supports wide variety of trajectory file formats.
pyPcazip can take one or many input MD trajectory files and converts them into a highly compressed, HDF5-based, .pcz format with insignificant loss of information.

In situ analysis can be simultaneously executed with MD simulation through which mitigates I/O bottlenecks.
Malakar et. al. \cite{Malakar-etal} provide approaches for improving in situ analysis performance. 
They study the scalability challenges of time- and space-shared modes of analyzing large-scale MD simulations through a topology-aware mapping for simulation and analysis using LAMMPS code.

All of the above frameworks, provide tools for parallel analysis of MD trajectories. 
However, none of these frameworks discusses the difficulties related to straggler tasks and their effect on the performance of parallel task.
The motivation behind present study is to provide parallel analysis in \package{MDAnalysis} library.
However, straggler tasks hamper achieving scalability beyond a single compute node \cite{Khoshlessan:2017ab}. 
Parallel computing requires substantial programming efforts especially when researchers are not experts in computer science.
As a result we decided to have a detailed study on the straggler problem (So called long Tail phenomena) and address useful solutions to overcome straggler problem.
Long Tail phenomena, whereby a small proportion of task stragglers significantly impede job completion time is a big challenge toward achieving improved performance \cite{Garraghan2016}.
Long Tail phenomena is a known problem in other frameworks such as Google\textsc{\char13}s MapReduce \cite{Dean2004}, Spark \cite{Kyong2017,Ousterhout2017,Gittens2016}, Hadoop \cite{Dean2004}, and cloud data centers \cite{Schmidt2016}. Straggler root-cause has both internal and external factors. 
Internal factors include heterogeneous capacity of worker nodes, and resource competition due to other tasks running on the same worker node. 
External factors include resource competition due to co-hosted applications, input data skew, remote input or output source being too slow and faulty hardware \cite{Chen2014}.

The straggler node due to the faulty hardware or mis-configuration can lead to stragglers \cite{Dean2004}. 
Garbage collections~\cite{Kyong2017,Ousterhout2017}, JVM positioning to cores~\cite{Kyong2017}, the delay's introduced while the task moves from the scheduler to executing~\cite{Gittens2016}, Disk I/O during shuffling, Java's just-in-time compilation~\cite{Ousterhout2017}, and output skew \cite{Ousterhout2017}. 
In addition to these reasons, stragglers on Spark have been attributed to the overall performance of the worker or competition between resources \cite{Yang2016}.
Garraghan et al. \cite{Garraghan2016} reported high CPU utilization, disk utilization, unhandled I/O access requests and network package loss as the most frequent reasons for stragglers on Virtualized Cloud Data-centers.

Tuning resource allocation and tuning parallelism such as breaking the workload into many small tasks that are dynamically scheduled at runtime \cite{Rosen2012}, slow Node-Threshold \cite{Dean2004}, speculative execution \cite{Dean2004}, sampling or data distribution estimation techniques, SkewTune to avoid data imbalance \cite{Kwon2012}, dynamic work rebalancing \cite{Schmidt2016}, blocked time analysis \cite{Ousterhout2015}, and Intelligent scheduling \cite{AWE-WQ2014} are among a wide variety of approaches that are trying to detect and mitigate stragglers. 
However, a comprehensive solution will involve tricks to avoid stragglers. 

In the present study, we find the root-cause of the straggler tasks and provide solutions through which we can improve performance and scaling.
Even though the proposed solutions to avoid stragglers, is specifically applied for MD trajectory analysis in \package{MDAnalysis} library, all of these principles and implementation techniques are potentially applicable to data analysis for other types of Python-based libraries.


