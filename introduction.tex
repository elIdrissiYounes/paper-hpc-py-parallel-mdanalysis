\label{sec:introduction}
Molecular dynamics (MD) simulations are a powerful method to generate new insights into the function of biomolecules \citep{Borhani:2012mi, Dror:2012cr, Orozco:2014dq, Perilla:2015kx, Bottaro:2018aa}.
Data analysis tools and libraries have been developed to extract the desired information from the output trajectories from MD simulations ~\cite{nmoldyn, nmoldyn-2012, Hum96, Hinsen:2000kx, Grant:2006ud, himach-2008, Romo:2009zr, Romo:2014bh, Michaud-Agrawal:2011fu, Gowers:2016aa, cpptraj-2013, mdtraj-2015, pteros2015, Doerr:2016aa}. \obnote{Be more to the point and make it relevant for the paper: talk about analyzing trajectories. Right now it just adds a bunch of citations}\mknote{Previously in discussion with giannis we decided to talk about straggler problem only and avoid discussing other packages. Do you mean not discussing the stragglers and talking about these papers in detail?}\obnote{Citing other packages is ok (I updated citations. But you need to explain the basics of what these packages do: analyzing trajectories --- see e.g. introduction to the SciPy paper.} \mknote{I added it. is that what you mean? Or do you mean that I need to explain each package specifically?}
\package{MDAnalysis} \citep{Gowers:2016aa,Michaud-Agrawal:2011fu} is an open-source object-oriented Python library for structural and temporal analysis of molecular dynamics simulation trajectories and individual protein structures.
Although \package{MDAnalysis} accelerates selected algorithms with Open-MP, full scale parallel trajectory analysis that could make use of modern HPC resources such as multiple nodes on a cluster has not yet been implemented in the library.

In our previous study, we used a parallel map-reduce approach to study the performance of a common task in the analysis of the structural dynamics of proteins, the root mean squared distance (RMSD) of the positions of all $C_{\alpha}$ atoms to their initial coordinates at time 0 \cite{Khoshlessan:2017ab, ICCP-2018}. 
We previously looked at the performance of \package{Dask} library \cite{Rocklin:2015aa}, and MPI, using the \package{mpi4py} package \cite{Dalcin:2011aa, Dalcin:2005aa}. 
For both Dask and MPI we found that our benchmark task only showed good strong scaling within a single node.
Distributed computing, allows parallelizing our problems for larger problem sizes and lead to performance gains.
But, as soon as we extend the computation beyond a single node, performance drops due to \emph{stragglers} tasks, a subset of processes that are significantly slower than the mean execution time of all tasks, increasing the total time to solution.
Stragglers significantly impede job completion time \cite{Garraghan2016} and have a high impact on performance and energy consumption on big data systems \cite{Tien-2017}.
In the present study, we analyze the MPI case in more detail to better understand the origin of the stragglers.
We want to provide simple and robust parallelization approaches to analyze molecular dynamics (MD) trajectories, in order to speed-up post-processing of MD trajectories in the \package{MDAnalysis} library. 

We have selected a commonly used algorithms in \package{MDAnalysis} RMSD.
We use the single program multiple data (SPMD) paradigm to parallelize this algorithm on HPC resources.
With SPMD, each process executes essentially the same code but on a different part of the data. 
We use Python, a machine-independent, byte-code interpreted, object-oriented programming (OOP) language, which is well-established in HPC parallel environments \cite{GAiN}. 
Based on our initial analysis, there are two important performance parameters, \text{$t_{Compute}$/$t_{IO}$} and \text{$t_{Compute}$/$t_{Communication}$} which are the ratio of computational to I/O load, as measured by the time spent on the computation versus the time spent on reading data from the file system, and the ratio of computational to communication load respectively that determines whether we observe stragglers.
If \text{$t_{Compute}$/$t_{IO}$}  $\gg 1$, the algorithm scales very well, otherwise it does not scale beyond a single node.  
For the algorithms with small \text{$t_{Compute}$/$t_{IO}$}, we need to come up with strategies to improve scaling and overcome straggler problems.
In addition, $t_{Compute}$/$t_{Communication}$ plays an important role on scaling and performance. \obnote{check, new} \mknote{I explained this effect in figure 5} 
The details on these are given in the results section. 
We noticed that communication and I/O are the two main scalability bottlenecks.

Taking advantage of Global Array toolkit \cite{GA, GAiN}, we were able to reduce communication cost noticeably.
In addition, our data show that I/O does not scale beyond a single node, which might be due to the contention of many requests to access the file and interference with MPI messages \cite{VMD2013, Kevin2018} \obnote{We have no proof that access to files interferes with MPI messages; without proof, you need to be more cautious (words like ``suggests'' or ``possibly'' or just leave it out. Generally speaking, not saying something that you are not sure  is a the default strategy for a scientific paper.}\mknote{we have citation for that. This papers I cited discuss this issue}. 
We examined two different approaches and both of them significantly improved the performance and lead near ideal scaling.
The code repository \url{https://github.com/hpcanalytics/supplement-hpc-py-parallel-mdanalysis} provides detailed documentation to recreate the computational environment on the tested HPC resources