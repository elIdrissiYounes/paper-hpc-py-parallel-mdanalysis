\label{sec:introduction}
Molecular dynamics (MD) simulations are a powerful method to generate new insights into the function of bio-molecules \cite{QwikMD2016}.
Data analysis tools and libraries have been developed to extract the desired information from these MD simulations ~\cite{Gowers:2016aa,Michaud-Agrawal:2011fu,cpptraj-2013,himach-2008,mdtraj-2015,Mura:2010mw}. \obnote{Be more to the point and make it relevant for the paper: talk about analyzing trajectories. Right now it just adds a bunch of citations}\mknote{These are addressed in more detail in background}
\package{MDAnalysis} \citep{Gowers:2016aa,Michaud-Agrawal:2011fu} is an open-source object-oriented Python library for structural and temporal analysis of molecular dynamics simulation trajectories and individual protein structures. 
Although \package{MDAnalysis} accelerates selected algorithms with Open-MP, full scale parallel trajectory analysis that could make use of modern HPC resources such as multiple nodes on a cluster has not yet been implemented in the library. 

In our previous study, we used a parallel map-reduce approach to study the performance of RMSD task \cite{Khoshlessan:2017ab, ICCP-2018}. 
We previously looked at the performance of \package{Dask} library \cite{Rocklin:2015aa}, and MPI, using the \package{mpi4py} package \cite{Dalcin:2011aa, Dalcin:2005aa}. 
For both Dask and MPI we found that our benchmark task only showed good strong scaling within a single node.
Distributed computing, allows parallelizing our problems for larger problem sizes and lead to performance gains.
But, as soon as we extend the computation beyond a single node, performance drops due to \emph{stragglers} tasks, a subset of processes that are significantly slower than the mean execution time of all tasks, increasing the total time to solution.
Stragglers significantly impede job completion time \cite{Garraghan2016} and have a high impact on performance and energy consumption on big data systems \cite{Tien-2017}.
In the present study, we analyze the MPI case in more detail to better understand the origin of the stragglers.
We want to provide simple and robust parallelization approaches to analyze molecular dynamics (MD) trajectories, in order to speed-up post-processing of MD trajectories in the \package{MDAnalysis} library. 

We have selected a commonly used algorithms in \package{MDAnalysis} RMSD.
We use the single program multiple data (SPMD) paradigm to parallelize this algorithm on HPC resources.
With SPMD, each process executes essentially the same code but on a different part of the data. 
We use Python, a machine-independent, byte-code interpreted, object-oriented programming (OOP) language, which is well-established in HPC parallel environments \cite{GAiN}. 
Based on our initial analysis, there are two important performance parameters, \text{$t_{Compute}$/$t_{IO}$} and \text{$t_{Compute}$/$t_{Communication}$} which are the ratio of computational to I/O load, as measured by the time spent on the computation versus the time spent on reading data from the file system, and the ratio of computational to communication load respectively that determines whether we observe stragglers.
If \text{$t_{Compute}$/$t_{IO}$}  $\gg 1$, the algorithm scales very well, otherwise it does not scale beyond a single node.  
For the algorithms with small \text{$t_{Compute}$/$t_{IO}$}, we need to come up with strategies to improve scaling and overcome straggler problems.
In addition, $t_{Compute}$/$t_{Communication}$ plays an important role on scaling and performance. \obnote{check, new} \mknote{I explained this effect in figure 5} 
The details on these are given in the results section. 
We noticed that communication and I/O are the two main scalability bottlenecks.

Taking advantage of Global Array toolkit \cite{GA, GAiN}, we were able to reduce communication cost noticeably.
In addition, our data show that I/O does not scale beyond a single node due to the contention of many requests to access the file and interference with MPI messages. 
We examined two different approaches and both of them significantly improved the performance and lead near ideal scaling.
The code repo ``https://github.com/hpcanalytics/supplement-hpc-py-parallel-mdanalysis" provides detailed documentation to recreate the computational environment on the tested HPC resources