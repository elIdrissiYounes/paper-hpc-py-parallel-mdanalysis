\label{sec:introduction}
Molecular dynamics (MD) simulations are a powerful method to generate new insights into the function of biomolecules \citep{Borhani:2012mi, Dror:2012cr, Orozco:2014dq, Perilla:2015kx, Bottaro:2018aa}.
These simulations produce trajectories, timeseries of atomic coordinates, that now routinely include millions of timesteps and can be Terabytes in size.
These trajectories need to be analyzed using statistical mechanics approaches \cite{Mura:2014kx} but because of the increasing size of the data, analysis of the trajectories is becoming a bottleneck in typical scientific workflows in the biomolecular simulation community \cite{Cheatham:2015}.
Many data analysis tools and libraries have been developed to extract the desired information from the output trajectories from MD simulations ~\cite{nmoldyn, nmoldyn-2012, Hum96, Hinsen:2000kx, Grant:2006ud, himach-2008, Romo:2009zr, Romo:2014bh, Michaud-Agrawal:2011fu, Gowers:2016aa, cpptraj-2013, mdtraj-2015, pteros2015, Doerr:2016aa} but few can make efficient use of modern high performance computing (HPC) resources to accelerate the analysis stage.
As a concrete example we focus on the \package{MDAnalysis} package \citep{Gowers:2016aa,Michaud-Agrawal:2011fu}, which is an open-source object-oriented Python library for structural and temporal analysis of molecular dynamics simulation trajectories and individual protein structures.
Although \package{MDAnalysis} accelerates selected algorithms with Open-MP, full scale parallel trajectory analysis has not yet been implemented in the library.
Here we discuss the challenges and lessons-learned for making parallel analysis on HPC resources feasible for a general purpose trajectory analysis library.

In our previous study, we used a parallel map-reduce approach to study the performance of a common task in the analysis of the structural dynamics of proteins, the calculation of the minimal root mean squared distance (RMSD) of the positions of a subset of atoms to a reference conformation under optimization of rigid body translations and rotations \cite{Khoshlessan:2017ab, ICCP-2018}, also known as ``RMSD fitting'' \cite{Liu:2010kx, Mura:2014kx}. 
We previously looked at the performance of the \package{Dask} library \cite{Rocklin:2015aa}, and MPI, using the \package{mpi4py} package \cite{Dalcin:2011aa, Dalcin:2005aa}. 
For both Dask and MPI we found that our benchmark task only showed good strong scaling within a single node.
Distributed computing, allows parallelizing our problems for larger problem sizes and lead to performance gains.
But, as soon as we extend the computation beyond a single node, performance drops due to \emph{stragglers} tasks, a subset of processes that are significantly slower than the mean execution time of all tasks, increasing the total time to solution.
Stragglers significantly impede job completion time \cite{Garraghan2016} and have a high impact on performance and energy consumption on big data systems \cite{Tien-2017}.
In the present study, we analyze the MPI case in more detail to better understand the origin of the stragglers.
We want to provide simple and robust parallelization approaches to analyze molecular dynamics (MD) trajectories, in order to speed-up post-processing of MD trajectories in the \package{MDAnalysis} library. 

As in our previous study we selected the commonly used RMSD algorithm implemented in \package{MDAnalysis} as a typical test case.
We use the single program multiple data (SPMD) paradigm to parallelize this algorithm on HPC resources.
With SPMD, each process executes essentially the same code but on a different part of the data. 
We use Python, a machine-independent, byte-code interpreted, object-oriented programming (OOP) language, which is well-established in HPC parallel environments \cite{GAiN}. 
We show that there are two important performance parameters, $t_{Compute}/t_{IO}$ and $t_{Compute}/t_{Communication}$ which are the ratio of computational to I/O load, as measured by the time spent on the computation versus the time spent on reading data from the file system, and the ratio of computational to communication load respectively that determine whether we observe stragglers.
If $t_{Compute}/t_{IO} \gg 1$, the algorithm scales very well, otherwise it does not scale beyond a single node.  
For the algorithms with small $t_{Compute}/t_{IO}$, we need to come up with strategies to improve scaling and overcome problems with stragglers.
In addition, $t_{Compute}/t_{Communication}$ plays an important role in determining scaling and performance.
We show that communication and I/O are the two main scalability bottlenecks.

Taking advantage of Global Array toolkit \cite{GA, GAiN}, we were able to reduce communication cost noticeably.
In addition, our data show that I/O does not scale beyond a single node, which might be due to the contention of many requests to access the file and interference with MPI messages \cite{VMD2013, Kevin2018} 
We examined two different approaches to mitigate I/O bottlenecks and both significantly improved the performance and lead to near ideal scaling.
The code repository \url{https://github.com/hpcanalytics/supplement-hpc-py-parallel-mdanalysis} provides detailed documentation to recreate the computational environment on the tested HPC resources