\label{impl_exp}

\subsection{RMSD Benchmarks}
\label{sec:RMSD}
RMSD algorithm for the present test case, represents a task for which computational workload per frame is smaller than I/O workload per frame ($t_{compute}^{frame}$ = 0.09 ms, $t_{IO}^{frame}$ = 0.3 ms, thus $\tcomp/\tIO \approx 0.3$). 
We showed previously that the RMSD task only scaled well up to 24 cores, on a single compute node on \emph{Comet}, \emph{Stampede}, and \emph{ASU computing resources}, using either Dask or MPI \cite{Khoshlessan:2017ab}. 
Here we focus on the MPI implementation (via \package{mpi4py} \cite{Dalcin:2011aa, Dalcin:2005aa}), in order to
better understand the cause for the poor scaling across nodes.

We observed stragglers, individual workers or MPI ranks, that take much longer to complete than mean execution time of all other workers or ranks. 
Waiting for these stragglers destroys strong scaling performance, as shown in Figure \ref{fig:MPIscaling}, \ref{fig:MPIspeedup} for MPI. 

In the example run in Figure \ref{fig:MPIranks}, ten ranks out of 72 take almost 65~s whereas the
remaining ranks only take about 40~s. The detailed breakdown of the time spent on each rank (Figure \ref{fig:MPIranks}) shows that time
for the actual computation, \tcomp, is fairly constant across ranks. 
The time spent on reading data from the shared trajectory file on the Lustre file system into memory, \tIO, shows variability across different ranks. 
Stragglers, however, appear to be defined by occasional much larger \emph{communication} times, \tcomm (line 16 in \ref{alg:RMSD}), that are on the order of 30~s in this example. 
For other ranks, \tcomm varies across different ranks and for a few ranks $\tcomm < 10$~s or is barely measurable. 
This initial analysis (especially Figure \ref{fig:MPIranks}) indicates that communication is a major issue. 

\begin{figure}[ht!]
\centering
\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/main-RMSD-t_total.pdf}
  \caption{Scaling total}
  \label{fig:MPIscaling}
\end{subfigure}
\hfill
\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/main-RMSD-speed_up.pdf}
  \caption{Speed-up}
  \label{fig:MPIspeedup}
\end{subfigure}
\bigskip

\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/main-RMSD-time_comp_IO_comparison.pdf}
\caption{\tcomp and \tIO scaling}
\label{fig:ScalingComputeIO}
\end{subfigure}
\hfill
\begin{subfigure} {.5\textwidth}
  \includegraphics[width=\linewidth]{figures/main-RMSD-BarPlot-rank-comparison_72_4.pdf}
  \caption{Time comparison on different parts of the calculations per MPI rank}
  \label{fig:MPIranks}
\end{subfigure}

\caption{Performance of the RMSD task with MPI which is I/O-bound $\tcomp/\tIO \approx 0.3$ on SDSC Comet.
Data are read from the file system (I/O included) and results are communicated back to
rank 0 (communications included). Five repeats are performed to collect statistics. (a-c) The error bars show
standard deviation with respect to mean. (d) Compute \tcomp, IO \tIO, communication \tcomm, ending the for loop $t_{end\_loop}$,
opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$,  $t_{Overhead2}$ per MPI rank (as described in methods).
This is typical data from one run of the 5 repeats. MPI ranks 0 and 63 to 72 are stragglers, i.e., their total time 
far exceeds the mean of the majority of ranks.}
  
\obnote{This does not look like t\_comp/t\_IO = 0.3 ? looks more like 2/30 = 0.06; the next page says 4/26=0.16. Please make sure that all numbers are consistent!}
\mknote{because t\_comp/t\_IO is per frame and it is also average, what I have reported is only average.}
\label{fig:MPIwithIO}
\end{figure} 

\subsubsection*{Identification of Scalability Bottleneck}

Figure \ref{fig:ScalingComputeIO} shows the scaling of \tcomp and \tIO individulally. 
As shown, \tcomp scales very well; however, \tIO does not show good scaling beyond a single node (24 cores) and that explains why we are seeing these variations in \tIO across different ranks (Figure \ref{fig:MPIranks}). 
Considering the results in Figures \ref{fig:MPIwithIO} and \ref{fig:ScalingComputeIO}, we can conclude that communication and I/O are the root causes for stragglers. 

\subsubsection*{Hardware}
\gpnote{Are the data in figure 2 from Comet and Stampede? Why Stampede and not SuperMic, Bridges, and Comet? Those are the system you say the experimentation was done.} \mknote{the data is on Comet. We perform everything on Comet as a baseline and then make a comparison across different clusters on the final section. We do not have any data from Stampede here}\gpnote{You say that you observed stragglers on Comet, Stampede and an ASU cluster. You do not show any data from Stampede, so I suggest to remove it. I think there are some data with stragglers from Bridges or SuperMic. Including those will make your point stronger.}\mknote{but I cited scipy paper. all the data are shown in scipy paper. Why do we need to repeat it? We can just cite it.}\gpnote{Sorry, Mahzad, but we cannot cite them. They were presented and discussed. Unless they have to offer something new, a different type of analysis than the SciPy paper, they should not be included apart from showing the motivation of this paper. That is my opinion and that is what I am doing in all my papers}\mknote{I do not agree. Here we are saying that we are observing stragglers on different resources including the ones used in the present study and the ones used for scipy. Then we want to discuss ways for overcoming those stragglers.}
We did not discern any specific patterns that could be traced to the underlying hardware. Stragglers were observed on \emph{SDSC Comet},
\emph{TACC Stampede} and \emph{ASU local computing resources} \cite{Khoshlessan:2017ab}. 
We also show in the present study that stragglers are observed on other XSEDE resources such as PSS Bridges and SuperMIC as well.
There was also no clear pattern in which certain MPI ranks would always be a straggler and we could also not trace stragglers to specific cores or nodes (or at least our data did not
reveal an obvious pattern). 
Therefore, the phenomenon of stragglers in the RMSD test appears to be independent from the resources.

\subsection{Dihedral Featurization Benchmarks}
\label{DF}
We briefly tested a much larger computational workload ($t_{compute_{per-frame}}$ = 40 ms, $t_{IO_{per-frame}}$ = 0.4 ms, thus $\tcomp/\tIO \approx 100$), namely dihedral
featurization on \emph{Comet} with Infiniband and Lustre file system.

The system scales linearly and close to ideal (Figure \ref{fig:MPIscaling-dihed}, \ref{fig:MPIspeedup-dihed}, \ref{fig:ScalingComputeIO-dihed}). Although, there is communication of large
result arrays (e.g. $\approx 2GB$ with 8 processes and $\approx 0.24GB$ with 72 processes), which is costly for multiple ranks (Figure \ref{fig:comparison-t_comm-dihedral}), the speed-up curve (Eq.~\ref{eq:speedup}) in Figure \ref{fig:MPIspeedup-dihed}
demonstrates very good scaling with the number of cores (up to 72 cores on 3 nodes).  The reason is that the communication cost (for
$MPI.Gather()$-line 39 in \ref{alg:Dihedral}) decreases with increasing the number of cores because the result array size that has to be
communicated also decreases (Figure \ref{fig:comparison-t_comm-dihedral}).  Based on Figure \ref{fig:comparison-t_comm-dihedral}, communication scales fairly well
with the number of processes. This can be attributed to larger array sizes compared to the RMSD task and according to \cite{Dalcin:2011aa}
the overhead of the \package{mpi4py} package decreases as the array size to be communicated increases. The dihedral featurization workload
has larger array size for all processor sizes (per task, a time series of feature vectors of size $N_{frames} \times (213 \times 2 \times 2$) when
compared to the RMSD workload (per task a float array of length $N_{frames}$) and therefore we are hypothesizing that the higher
performance of \package{mpi4py} for larger array sizes has lead to better overall scaling. 
In addition, for higher computational workloads the competition over accessing the file is less severe as compared to lower computational workloads. 
 
\begin{figure}[ht!]
\centering
\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/main-dihedral-t_total.pdf}
  \caption{Scaling total}
  \label{fig:MPIscaling-dihed}
\end{subfigure}
\hfill
\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/main-dihedral-speed_up.pdf}
  \caption{Speed-up}
  \label{fig:MPIspeedup-dihed}
\end{subfigure}
\bigskip

\begin{subfigure} {0.4\textwidth}
  \includegraphics[width=\linewidth]{figures/main-dihed-time_comp_IO_comparison.pdf}
\caption{\tcomp and \tIO scaling}
\label{fig:ScalingComputeIO-dihed}
\end{subfigure}
\hfill
\begin{subfigure} {.5\textwidth}
  \includegraphics[width=\linewidth]{figures/main-dihedral-BarPlot-rank-comparison_72_5.pdf}
  \caption{Time comparison on different parts of the calculations per MPI rank}
  \label{fig:MPIranks-dihed}
\end{subfigure}

\caption{Performance for the \textbf{dihedral featurization} workload,
which is compute-bound $\tcomp/\tIO \approx 100$ on SDSC Comet. Data are read from the file system (I/O included) 
and results are communicated back to rank 0 (communications included). Five repeats are performed to collect statistics. 
(a-c) The error bars show standard deviation with respect to mean. (d) Compute \tcomp, IO \tIO, communication \tcomm, ending the for loop $t_{end\_loop}$,
  opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$,  $t_{Overhead2}$ per MPI rank (as described in methods).
  This is typical data from one run of the 5 repeats. No straggler is observed.} 
\label{fig:MPIwithIO-dihed}
\end{figure} 

\begin{figure}[ht!]
\begin{subfigure} {.55\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/ViolinPlot-Ncores-comparison-comm-dihedral.pdf}
  \caption{Communication time distribution for different number of
    processes}
\end{subfigure}
\hfill
\begin{subfigure}{.35\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/t_comm_mean-dihedral.pdf}
  \caption{Communication time scaling}
\end{subfigure}
\caption{Comparison of communication cost for different number of processes over 5 repeats for the \textbf{dihedral featurization}
  workload performed on SDSC Comet (a) Communication time distribution shown as violin plots \protect\cite{Hintze:1998tw}
    with kernel density estimates as implemented in \package{seaborn} for different number of processes; the white dot indicates the median. (b) Scaling communication time (mean and standard deviation).
  \gpnote{I am really confused with this figure. How many GBs of data did you move and there was a delay of 1.5 hours? Was it more 10TBs? (Assuming you were able to use 20Gbps from IB $20Gbps*5000sec=10^{14}=12500000000000B=12500000000000/2^{40}TB\simeq11.6TB$)}} \mknote{with 8 processes size of the data to be communicated is 2 GB per process. But communication time is not uniform across different processes and for some processes it is in the order of 1000 for another processes is 0.86 and the other is 4 s. Thus there are stragglers due to communication but because communication is small with respect to compute it does not affect the performance.}
  \gpnote{This is very weird. Are you sure that the IB network was used correctly? Over 1k seconds for 2GB is way too much. I am afraid
  that the IB network was not used correctly, either from the implementation of RMSD or from OpenMPI's custom installation. I will also ask AndreM, who has worked a bit with OpenMPI. Either way, it does not make any sense to me.}
  \mknote{I do not think there is any problem with IB usage. I ran dihedral featurzation at two different time (one several months after the other) and every time I did five repeats. Both gave me the same pattern and results.}
  \gpnote{Then something is very wrong with the setup of our experiments. 2GBs on a network of 56 Gbps should take a few seconds or worst case scenario minutes to transfer not hours. Are you sure these are not milliseconds?}
  \mknote{Please refer to the email discussion for my answer to this}
\label{fig:comparison-t_comm-dihedral}
\end{figure}

Overall, increasing the computational workload over I/O improves scaling. For large compute-bound workloads such as the dihedral
featurization task, stragglers are eliminated and nearly ideal strong scaling is achieved. 
The fact that linear scaling is possible, even with expensive communications, makes parallelization a valuable strategy to reduce
the total time to solution for large computational workloads. In a real-world application to one of our existing data sets on local
workstations with Gigabit interconnect and Network File System (NFS) (using the Dask parallel library instead of MPI), analysis time was reduced from more
than a week to a few hours (data not shown).

\subsection{Effect of \tcomp/\tIO on Performance}
\label{bound}

The RMSD task turned out to be I/O bound, i.e.,
\begin{gather*}
  \frac{\tcomp}{\tIO} \ll 1.
\end{gather*}

and we were not able to achieve good scaling above a single node. 
However, Dihedral featurization turned out to be compute bound and we were able to achieve near ideal scaling. 
We therefore, hypothesized that decreasing the relative I/O load with respect to compute load would also reduce the stragglers. 
We therefore increased the computational load so that the work became compute bound, i.e.,

\begin{gather*}
  \frac{\tcomp}{\tIO} \gg 1.
\end{gather*}

i.e., now processes are not constantly performing I/O and instead, I/O is interleaved with longer periods of computation.
In order to artificially increase the computational load we repeated the same RMSD calculation (line 10, algorithm \ref{alg:RMSD}) 40, 70 and 100 times in a loop respectively.

\subsubsection{Increased workload (RMSD)}
The RMSD workload was artificially increased forrty-fold (``$40\times$'' ), seventy-fold (``$70\times$'' ), and hundred-fold (``$100\times$'' ) and we measured performance as before. 
These workloads correspond to \tcomp/\tIO ratio of 12, 21, 30 respectively as shown in Table \ref{tab:load-ratio}.
We performed this experiment to show the effect of $\tcomp/\tIO$ ratio on performance (Figure \ref{fig:tcomp_tIO_effect}).
On average, each rank\textsc{\char13}s workload is $N_{frames} \times \tIO $ (where $N_{frames}=N_{frames}^{total}/N$ is the
number of frames per trajectory block, i.e., the number of frames
processed by each MPI rank for $N$ processes) for I/O, and
$X \times N_{frames} \times \tcomp$ for the RMSD calculation. 
$X$ is the factor by which we increase the RMSD compute workload in our experiment.

\begin{table}[ht!]
\centering
\begin{tabular}{c c}
  \toprule
           \bfseries\thead{Workload} & \bfseries\thead{$\tcomp/\tIO$}\\
  \midrule
    RMSD $1\times$ & 0.3\\  
    RMSD $40\times$ & 12\\    
    RMSD $70\times$ & 21\\  
    RMSD $100\times$ & 30\\  
  \bottomrule
\end{tabular}
\caption[Change in load-ratio with RMSD workload]
{Change in \tcomp/\tIO ratio with change in the RMSD workload. We artificially increased the RMSD workload in order to
examine the effect of compute to I/O ratio on the performance.}
\label{tab:load-ratio}
\end{table}

As the $\tcomp/\tIO$ ratio increases, speed-up and performance improves and 
show overall better scaling than the I/O-bound workload, i.e. $1\times$ RMSD (Figure \ref{fig:S1_tcomp_tIO_effect}).
When $\tcomp/\tIO$ ratio increases, the RMSD calculation consistently scales up to larger numbers of cores ($N=56$ for $70\times$ RMSD).
Figures \ref{fig:S2_tcomp_tIO_effect} and \ref{fig:E_tcomp_tIO_effect} shows the improvement in performance more clearly.
In fact, as the $\tcomp/\tIO$ ratio increases, the values of speed-up and efficiency get closer to their ideal value for each number of processor count.  

\begin{figure}[ht!]
\centering
\begin{subfigure} {.3\textwidth}
  \includegraphics[width=\linewidth]{figures/Compute_to_IO_ratio_on_performance_2d_v17.pdf}
  \caption{Speed-Up}
  \label{fig:S1_tcomp_tIO_effect}
\end{subfigure}
\hfill
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{figures/Compute_to_IO_ratio_on_performance_2d_2_v17.pdf}
  \caption{Speed-Up}
  \label{fig:S2_tcomp_tIO_effect}
\end{subfigure}
\hfill
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{figures/Compute_to_IO_ratio_on_performance_2d_3_v17.pdf}
  \caption{Efficiency}
  \label{fig:E_tcomp_tIO_effect}
\end{subfigure}
%
\caption{Effect of $\tcomp/\tIO$ ratio on performance of the RMSD task with MPI performed on SDSC Comet. We tested performance for $\tcomp/\tIO$ ratios of 0.3, 12, 21, 30
which correspond to $1\times$ RMSD, $40\times$ RMSD, $70\times$ RMSD, and $100\times$ RMSD respectively (communication is included). (a) Effect of \tcomp/\tIO on the Speed-Up
(b) Change in the Speed-Up with respect to \tcomp/\tIO for different processor counts (c) Change in the efficiency with respect to \tcomp/\tIO for different processor counts}
\label{fig:tcomp_tIO_effect}
\end{figure}

Even for moderately compute-bound workloads such
as the $40\times$ and $70\times$ RMSD tasks, increasing the computational workload over I/O reduced the impact of stragglers even
though they still contribute to large variations in timing across different ranks and thus irregular scaling due to the fluctuations.

Given the results for Dihedral featurization and RMSD algorithms (Algorithms \ref{alg:Dihedral}, and \ref{alg:RMSD}) and $X\times$ RMSD (Figure \ref{fig:tcomp_tIO_effect})
we hypothesize that MPI competes with Lustre on the same network interface, which would explain why communication appears to
be primarily a problem in the presence of I/O when \tcomp/\tIO is small.
In fact, decreasing the I/O load relative to the compute load should open up the network for communication. 

\subsection{Communication Cost and Application of Global Array}
\label{Global-Array}
As discussed in the previous sections, Figure \ref{fig:MPIranks}, for small \tcomp/\tIO communication acts as the scalability bottleneck. 
In fact, when the processes communicate result arrays back to the master process (rank 0), some processes take much longer as compared to other processes. 
Now we want to know what strategies can be used to avoid communication cost. 

We used global array instead of collective communication in MPI and examined the change in the performance. 
In global array, we define one large RMSD array, and each MPI rank updates its associated block in the global RMSD array using $ga\_put$. 
At the end, when all the processes exit \texttt{block\_rmsd()} function and update their local block in the global array, rank 0 will access the whole global array using $ga\_access$.
In fact, in global arrays the time for communication is $t_{ga\_put}+t_{ga\_access}$.
Given the speed up plots (Figure \ref{fig:MPIspeedup-ga4py}) and total time scaling (Figure \ref{fig:MPIscaling-ga4py}) global array improves strong scaling performance.
Although communication time has significantly decreased using global array (compare Figure \ref{fig:MPIranks-ga4py} to Figure \ref{fig:MPIranks}),
the existing variation in the dominant I/O part of the calculation plus two delayed MPI ranks due to the delay in opening the trajectory would still prevent ideal scaling (Figure \ref{fig:ScalingComputeIO-ga4py}).
This figure shows only one repeat out of 5 we performed for our benchmark study. 
Opening the trajectory was not a problem in other repeats.
In fact, although communications were performed using global arrays, scaling is still far from ideal as a result of slow processes due to I/O variation and the delay in opening the trajectory.
\obnote{In Fig 8c it is the trajectory opening step that creates stragglers. This was not a problem before. Is this now ALWAYS the problem when using GA? Needs to be discussed. }
\mknote{no only happened in one repeat. Does not seem to me to be a problem with GA}
These slow processes take about 50~s, which are slower than the mean execution time of all ranks, i.e. 17~s. 

\begin{figure}[ht!]
\centering
\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/RMSD-ga4py-t_total.pdf}
  \caption{Scaling total}
  \label{fig:MPIscaling-ga4py}
\end{subfigure}
\hfill
\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/RMSD-ga4py-speed_up.pdf}
  \caption{Speed-up}
  \label{fig:MPIspeedup-ga4py}
\end{subfigure}
\bigskip

\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/RMSD-ga4py-time_IO_comparison.pdf}
\caption{\tcomp and \tIO scaling}
\label{fig:ScalingComputeIO-ga4py}
\end{subfigure}
\hfill
\begin{subfigure} {.5\textwidth}
  \includegraphics[width=\linewidth]{figures/RMSD-ga4py-BarPlot-rank-comparison_72_1.pdf}
  \caption{Time comparison on different parts of the calculations per MPI rank}
  \label{fig:MPIranks-ga4py}
\end{subfigure}

\caption{Performance of the RMSD task with MPI using global array ($\tcomp/\tIO \approx 0.3$) on SDSC Comet.
Data are read from the file system (I/O included). All ranks update the global array ($ga_{put}$) and rank 0 accesses the whole RMSD array through the global memory address ($ga_{get}$) (communications included).
Five repeats are performed to collect statistics. (a-c) The error bars show standard deviation with respect to mean. 
(d) Compute \tcomp, IO \tIO, communication \tcomm, access to the whole global array by rank 0 $t_{Access\_Global\_Array}$, ending the for loop $t_{end\_loop}$, 
opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$, 
$t_{Overhead2}$ per MPI rank (as described in methods). This is typical data from one run of the 5 repeatsMPI ranks 20 and 56 are stragglers, i.e., 
their total time far exceeds the mean of the majority of ranks.}
\label{fig:MPIwithIO-ga4py}
\end{figure}

\subsection{I/O Cost}
\label{I/O}
We showed previously that the I/O system can have a large effect on the parallel performance of the RMSD task \cite{Khoshlessan:2017ab},
especially because the average time to perform the computation \tcomp (about 0.09 ms) is about three times smaller than the I/O time \tIO (about 0.3 ms) (Figures \ref{fig:ScalingComputeIO} and \ref{fig:MPIwithIO}). 
In fact, poor I/O performance is responsible for the stragglers, and the question is ``are stragglers waiting for file access?". 
Due to the large file size and memory limit, processes are not able to load the whole trajectory into memory at once and as a result each process is only allowed to load one frame into memory at a time.
The test trajectory has about $2,512,200$ frames in total and as a result there will be $2,512,200$ file access requests. 
Thus, when the compute time is small with respect to I/O, then I/O can be a major issue as we also see in our results (Figures \ref{fig:ScalingComputeIO} and \ref{fig:MPIwithIO}).    
Read throughput might be limited by the available bandwidth on the Infini-band network interface that serves the Lustre file system and access to files might be throttled.
We show that depending on the cluster and its capabilities the throughput might become a problem for achieving good performance and we also show ways to overcome this problem and improve performance.
In fact, we need to come up with ways and strategies to avoid the competition over file access across different ranks when \tcomp/\tIO ratio is small.
To this aim, we experimented two different ways for reducing I/O cost and examined their effect on the performance.
These two ways include: Splitting the trajectory file into as many segments as number of processes and MPI-based Parallel HDF5.
We discuss these two approaches in detail in the following sections.

\subsubsection{Splitting the Trajectories}
\label{splitting-traj}
In all the previous benchmarks all processes were using a shared trajectory file.
In order to test whether \emph{I/O and communication compete over the network resources with small \tcomp/\tIO ratio}, we splitted our trajectory file into as many trajectory segments as the number of processes.
This means that if we have $N$ processes, the trajectory file is splitted into $N$ segments and each segment will have $N_{b}$ frames in it. 
Through this approach, each process will have access to its own segment and there will be no competition over file accesses. 
For reference, the necessary time for splitting the trajectory file is given in \ref{sec:splitting-timing}.

\subsubsection*{Performance without Global Array}
We ran a benchmark up to 8 nodes (192 cores) and, we observed rather better scaling behavior with efficiencies above 0.6 (Figure \ref{fig:MPIscaling-split} and \ref{fig:MPIspeedup-split}) and the delay time for stragglers has also reduced from 65s to about 10s for 72 processes. 
However, the scaling is still far from ideal due to the communication. 
Although the delay due to communication is much smaller as compared to RMSD with a shared trajectory file (Compare Figure \ref{fig:MPIranks-split} to Figure \ref{fig:MPIranks}), it is still delaying several processes and as a result leads to longer job completion time (Figure \ref{fig:MPIranks-split}). 
These delayed tasks impact performance as well and hence the speed-up is not still close to ideal scaling (Figure \ref{fig:MPIspeedup-split}).

\begin{figure}[ht!]
\centering
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{figures/Comparison_IO_compute_scaling_traj_splitting.pdf}
  \caption{Scaling total}
  \label{fig:ScalingComputeIO-split}
\end{subfigure}
\hfill
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{figures/Comparison_tot_time_traj_splitting.pdf}
  \caption{Scaling total}
  \label{fig:MPIscaling-split}
\end{subfigure}
\hfill
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{figures/Comparison_Speed_UP_traj_splitting.pdf}
  \caption{Speed-up}
  \label{fig:MPIspeedup-split}
\end{subfigure}
\bigskip

\begin{subfigure} {.45\textwidth}
  \includegraphics[width=\linewidth]{figures/split-BarPlot-rank-comparison_192_5.pdf}
   \caption{Time comparison on different parts of the calculations per MPI rank without global array}
  \label{fig:MPIranks-split}
\end{subfigure}
\hfill
\begin{subfigure} {.45\textwidth}
  \includegraphics[width=\linewidth]{figures/split-ga-BarPlot-rank-comparison_192_5.pdf}
  \caption{Time comparison on different parts of the calculations per MPI rank using global array}
  \label{fig:MPIranks-split-ga}
\end{subfigure}

\caption{Comparison on the performance of the RMSD task with MPI when the trajectories are splitted using global array and without global array ($\tcomp/\tIO \approx 0.3$) on SDSC Comet.
Data are read from the file system (I/O included). In case of global array, all ranks update the global array ($ga_{put}$) and rank 0 accesses the whole RMSD array through the global memory address ($ga_{get}$).
Five repeats are performed to collect statistics. (a-c) The error bars show standard deviation with respect to mean. 
(d-e) Compute \tcomp, IO \tIO, communication \tcomm, access to the whole global array by rank 0 $t_{Access\_Global\_Array}$, ending the for loop $t_{end\_loop}$, 
opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$, $t_{Overhead2}$ per MPI rank (as described in methods). When global array is not used, the performance is affected due to the non-uniform
communication time across different ranks. However, when global array is used communication time has significantly reduced and scaling is close to ideal.}
\label{fig:MPIwithIO-split}
\obnote{The compute/IO ratio is about 1:4 judging from the graph, i.e., ~0.25 ? or did you calculate the ratio for the serial version?}
\mknote{Yes, this is based on serial version}
\end{figure}

\subsubsection*{Performance using Global Array}
\gpnote{This is very confusing. In a previous paragraph you say that GA passes data over the IB network. Here you say there is no congestion? Don't they, MPI communication, MPI I/O, and GA communication, use the same network? Also, I do not think you have the data to support that clearly, unless you know how many Bytes are moved around, how many packets and their type. If that is the case include it. Or are you using the Local filesystem on the nodes and IO has nothing to do with the IB?}
\mknote{I think there is something different in the way GA use IB that when we use GA there is no contention. That is mainly because there is no synchronization and that reduces the traffic a lot. GA uses IB but the way it uses is very different from MPI Gather. it defines a pointer and that is very different from what happens in MPI for communication.}
\gpnote{So there are no similarities between GA communication and asynchronous MPI communication? Asynchronous MPI communication does not block anything and there is no need for synchronization when done correctly. It is still confusing for me at least.}\mknote{No,  there are no similarities. MPI.Gather is collective communication and collective communications are blocking in MPI, right?}\gpnote{For synchronous, yes you are right. I am asking about asynchronous MPI communication.}\mknote{Maybe using Isend and Ireceive can be helpful but I do not think it will be comparable with GA. DO you volunteer to give it a shot and replace mpi.Gather with asynchronous communication and run it so that we can compare with GA?}
Previously, we showed that global array significantly reduces the communication cost (Section \ref{Global-Array}). 
We want to see how the performance looks like if we split our trajectory file and take advantage of global array as well.
Again, we ran our benchmark up to 8 nodes (192 cores) and, we observed near ideal scaling behavior with efficiencies above 0.9 (Figure \ref{fig:MPIscaling-split} and \ref{fig:MPIspeedup-split}) with no straggler tasks (Figure \ref{fig:MPIranks-split-ga}).  
The present results show that contention for a file impacts the performance. 
The initial results with splitting the trajectory file suggests that there is in fact an effect, which possibly also interferes with the communications when $\tcomp/\tIO \ll 1$ (i.e. with a I/O bound workload).

\subsection{Effect of \tcomp/\tcomm on Performance}

In addition to the compute to I/O ratio we define another important performance parameter which is compute to communication ratio.
Based on the results shown in the section \ref{I/O}; although we overcame the I/O effect by splitting the trajectory, scaling is still far from ideal without global array.
This means that the task remains communication bound (Figure \ref{fig:tcomp_tcomm_ratio_rmsd}). 
Examples of the communication bound task is RMSD task with splitting the trajectories and without global array.
When the task is communication bound, i.e.,

\begin{gather*}
  \frac{\bar{t}_{comp}}{\bar{t}_{comm}} \ll 1.
\end{gather*}

We are not able to achieve near ideal scaling even when I/O is not a bottleneck anymore (Figure \ref{fig:MPIwithIO-split}).
However, Dihedral featurization turned out to be compute bound and we were able to achieve near ideal scaling even when we did not use global array in our benchmark (Figure \ref{fig:MPIwithIO-dihed}).
This is mainly because both communication and I/O are very small with respect to compute time ($\tcomp/\tIO \approx 100$ and Figure \ref{fig:tcomp_tcomm_ratio_dihed}). 
Although, there are stragglers due to communication (Figure \ref{fig:MPIwithIO-comm-dihed-rank}) their effect on performance is not crucial mainly because compute to communication ratio is very large. 
We therefore, conclude that decreasing the relative communication load with respect to compute load would also reduce the stragglers. 
If the computational load is more than communication load then the work became compute bound, i.e.,

\begin{gather*}
  \frac{\bar{t}_{comp}}{\bar{t}_{comm}} \gg 1.
\end{gather*}

It should be noted that size of the RMSD arrays to be communicated to rank 0 is much smaller than the resulted arrays in Dihedral featurization task (0.7 MB versus 2GB with 8 processes).
As a result communication time is very small within a single node (24 cores) for RMSD task and that is why \tcomp/\tcomm $\gg$ 1.
However for more than a single node, communication overhead becomes a bottleneck and \tcomp/\tcomm $\ll$ 1 (Figure \ref{fig:tcomp_tcomm_ratio_rmsd}).

\begin{figure}[ht!]
\centering
\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/tcomp_tcomm_ratio_dihed.pdf}
  \caption{Dihedral Featurization}
  \label{fig:tcomp_tcomm_ratio_dihed}
\end{subfigure}
\bigskip
\begin{subfigure} {.4\textwidth}
  \includegraphics[width=\linewidth]{figures/tcomp_tcomm_RMSD.pdf}
  \caption{RMSD}
  \label{fig:tcomp_tcomm_ratio_rmsd}
\end{subfigure}
%
\caption{Average compute to communication ratio with number of processes for RMSD and Dihedral featurization tasks performed on SDSC Comet.
Five repeats are performed to collect statistics and error bars show standard deviation with respect to mean.}
\label{fig:tcomp_tcomm_ratio}
\end{figure} 

\begin{figure}[ht!]
\centering
\begin{subfigure}{.47\textwidth}
  \includegraphics[width=\linewidth]{figures/main-dihedral-BarPlot-rank-comm-comparison_8_5.pdf}
  \caption{8 Processes}
  \label{fig:comm-rank-dihed-8}
\end{subfigure}
\bigskip
\begin{subfigure} {.47\textwidth}
  \includegraphics[width=\linewidth]{figures/main-dihedral-BarPlot-rank-comm-comparison_72_5.pdf}
  \caption{72 Processes}
  \label{fig:comm-rank-dihed-72}
\end{subfigure}
%
\caption{Examples of timing per MPI rank for Dihedral featurization task ($\tcomp/\tIO \approx 100$) on SDSC Comet without showing the compute portion with 8 and 72 processes.
I/O \tIO, communication \tcomm, ending the for loop $t_{end\_loop}$, opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$,  
$t_{Overhead2}$ per MPI rank (as described in methods).
This task is compute bound and the compute portion is not shown here so that we can observe the variation in communication time across different ranks. 
In fact, there are stragglers due to communication and communication time is not uniform across different ranks. 
However, the performance is not affected due to stragglers caused by communication because the task is compute bound.
Five repeats are performed to collect statistics and this is typical data from one run of the 5 repeats.}
\label{fig:MPIwithIO-comm-dihed-rank}
\end{figure} 

\subsubsection{ChainReader}
ChainReader is used in \package{MDAnalysis} to represent multiple trajectories as one virtual trajectory. 
In Section \ref{splitting-traj} we showed how splitting the trajectories will help to overcome I/O and improves scaling. 
However, the number of trajectories may not necessarily be equal to the number of processes.
Specially, trajectories from MD simulations on HPC resources are kept in small chunks and making sure that the number of processes is equal to the number of trajectory files will not be feasible for the typical users. 
Here, we use ChainReader in MDAnalysis to read all the trajectories by each process and we want to see how it will affect the performance. 
In fact, using ChainReader each process opens all the trajectories but I/O will only happen from a specific block of the trajectory specific to that process only.
 
 \begin{figure}[ht!]
\centering
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{figures/Comparison_IO_compute_scaling_traj_splitting-chain-reader.pdf}
  \caption{\tcomp and \tIO scaling}
  \label{fig:MPIscaling-chain-reader}
\end{subfigure}
\hfill
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{figures/Comparison_tot_time_traj_splitting-chain-reader.pdf}
  \caption{Scaling total}
  \label{fig:MPItottime-chain-reader}
\end{subfigure}
\hfill
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{figures/Comparison_Speed_UP_traj_splitting-chain-reader.pdf}
  \caption{Speed-up}
  \label{fig:MPIspeedup-chain-reader}
\end{subfigure}
\bigskip

\begin{subfigure} {.45\textwidth}
  \includegraphics[width=\linewidth]{figures/chain-reader-no-ga-BarPlot-rank-comparison_192_5.pdf}
   \caption{Time comparison on different parts of the calculations per MPI rank using ChainReader without global array}
  \label{fig:MPIranks-split}
\end{subfigure}
\hfill
\begin{subfigure} {.45\textwidth}
  \includegraphics[width=\linewidth]{figures/chain-reader-ga-BarPlot-rank-comparison_192_3.pdf}
  \caption{Time comparison on different parts of the calculations per MPI rank using ChainReader using global array}
  \label{fig:MPIranks-split-ga}
\end{subfigure}

\caption{Comparison on the performance of ChainReader for RMSD task with MPI when the trajectories are splitted using global array and without global array ($\tcomp/\tIO \approx 0.3$) on SDSC Comet.
Data are read from the file system (I/O included). In case of global array, all ranks update the global array ($ga_{put}$) and rank 0 accesses the whole RMSD array through the global memory address ($ga_{get}$).
Five repeats are performed to collect statistics. (a-c) The error bars show standard deviation with respect to mean. 
(d-e) Compute \tcomp, IO \tIO, communication \tcomm, access to the whole global array by rank 0 $t_{Access\_Global\_Array}$, ending the for loop $t_{end\_loop}$, 
opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$, $t_{Overhead2}$ per MPI rank (as described in methods). When global array is not used, the performance is affected due to the non-uniform
communication time across different ranks. However, when global array is used communication time has significantly reduced and scaling is close to ideal. In addition, time for ending the for loop $t_{end\_loop}$ and 
opening the trajectory $t_{opening\_trajectory}$ is a bottleneck as opposed to our calculation without ChainReader. We obtained the results for the ChainReader with a \emph{patched version} of a MDAnalysis that avoid race condition}
\label{fig:MPIwithIO-split-chain-reader}
\end{figure}

XDR based readers store persistent offsets on disk. 
The offsets are used to enable access to random frames efficiently. 
These offsets will be generated automatically the first time the trajectory is opened and are stored in hidden $\ast.xtc\_offsets.npz$ files. 
The advantage of these offset files is that after opening the trajectory for the first time, opening the same file will be very fast afterward. 

It sometimes can happen that the stored offsets get out off sync with the trajectory they refer to. 
For this the offsets also store the number of atoms, size of the file and last modification time. 
If any of these parameters change, the offsets are recalculated. 
 
If the XTC changes and any of the number of atoms, size of the file and last modification time changes but the offset file is not updated, then the offset file can be detected as invalid.
With ChainReader in parallel, each process opens all the trajectories; therefore in case of invalid offset file, several processes might want to recalculate these parameters and rebuild the offset file which will lead to race condition.
In order to avoid race condition, we removed this check from MDAnalysis, but this comes at the price of not checking the validity of the offset file at all.
 
We obtained the results for the ChainReader with a \emph{patched version} of \package{MDAnalysis} that eliminates the race condition by assuming that XTC index files are present and valid. 
Figure \ref{fig:MPIwithIO-split-chain-reader} shows the results for performance of ChainReader for RMSD task using GA and without GA. 
As shown in Figure \ref{fig:MPIspeedup-chain-reader} the cases with GA and without GA scale up to 144 and 92 cores respectively.
The scaling is not close to ideal as opposed to what we achieved in Section \ref{splitting-traj}. 
However, in this case I/O and compute time are scaling very well (Figure \ref{fig:MPIscaling-chain-reader} as compared to Figure \ref{fig:ScalingComputeIO-split}) and the time for ending the for loop $t_{end\_loop}$ and opening the trajectory $t_{opening\_trajectory}$ appear to be the performance bottleneck as opposed to the results shown in Section \ref{splitting-traj} (Figures \ref{fig:MPIranks-split} and \ref{fig:MPIranks-split-ga}). 
The reason for which $t_{end\_loop}$ and $t_{opening\_trajectory}$ increases are not very well clear to us, but this will be studied in our future work in detail.

\subsubsection{MPI-based Parallel HDF5}
\label{HDF5}
Another approach we examined to improve I/O scaling is MPI-based Parallel HDF5. 
We converted our XTC trajectory file into HDF5 format so that we can test the performance of parallel IO with HDF5 file format.
The time it took to convert our XTC file with $2,512,200$ frames into HDF5 format was about $5400~s$ in our local resources with network file system (NFS).
Again, we ran our benchmark up to 8 nodes (192 cores) and, we observed near ideal scaling behavior with efficiencies above 0.8 (Figure \ref{fig:MPIscaling-hdf5} and \ref{fig:MPIspeedup-hdf5}) with no straggler tasks (Figure \ref{fig:MPIranks-hdf5}).  
When we split our trajectory, scaling is better as compared to that of parallel I/O (Compare Figure \ref{fig:MPIspeedup-hdf5} to Figure \ref{fig:MPIspeedup-split}). 
However, both methods scale very well up to 8 nodes and have comparable performance.  

\begin{figure}[ht!]
\centering
\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/hdf5-t_total.pdf}
  \caption{Scaling total}
  \label{fig:MPIscaling-hdf5}
\end{subfigure}
\hfill
\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/hdf5-speed_up.pdf}
  \caption{Speed-up}
  \label{fig:MPIspeedup-hdf5}
\end{subfigure}
\bigskip

\begin{subfigure}{.4\textwidth}
\centering
  \includegraphics[width=\linewidth]{figures/hdf5-time_IO_comparison.pdf}
\caption{\tcomp and \tIO scaling}
\label{fig:ScalingComputeIO-hdf5}
\end{subfigure}
\hfill
\begin{subfigure} {.5\textwidth}
  \includegraphics[width=\linewidth]{figures/hdf5-BarPlot-rank-comparison_192_4.pdf}
  \caption{Time comparison on different parts of the calculations per MPI rank}
  \label{fig:MPIranks-hdf5}
\end{subfigure}
%
\caption{Performance of the RMSD task with MPI-based parallel HDF5 ($\tcomp/\tIO \approx 0.3$) on SDSC Comet.
Data are read from the file system from a shared HDF5 file format instead of XTC format (Independent I/O) and results are communicated back to rank 0 (communications included). 
Five repeats are performed to collect statistics. (a-c) The error bars show standard deviation with respect to mean. (d) Compute \tcomp, IO \tIO, communication \tcomm, ending the for loop $t_{end\_loop}$,
  opening the trajectory $t_{opening\_trajectory}$, and overheads $t_{Overhead1}$,  $t_{Overhead2}$ per MPI rank (as described in methods).
  This is typical data from one run of the 5 repeats. No straggler is observed.}
\label{fig:MPIwithIO-hdf5}
\end{figure}
